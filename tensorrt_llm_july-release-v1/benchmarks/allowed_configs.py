_allowed_configs = {
    "gpt_350m": {
        "num_layers": 24,
        "num_heads": 16,
        "hidden_size": 1024,
        "vocab_size": 51200,
        "hidden_act": 'gelu',
        "n_positions": 1024,
        "max_batch_size": 256,
        "max_input_len": 512,
        "max_output_len": 200,
        "builder_opt": None,
    },
    "gpt_175b": {
        "num_layers": 96,
        "num_heads": 96,
        "hidden_size": 12288,
        "vocab_size": 51200,
        "hidden_act": 'gelu',
        "n_positions": 2048,
        "max_batch_size": 64,
        "max_input_len": 512,
        "max_output_len": 200,
        "builder_opt": None,
    },
    "gpt_350m_sq_per_tensor": {
        "num_layers": 24,
        "num_heads": 16,
        "hidden_size": 1024,
        "vocab_size": 51200,
        "hidden_act": 'gelu',
        "n_positions": 1024,
        "max_batch_size": 256,
        "max_input_len": 512,
        "max_output_len": 200,
        "builder_opt": None,
        "use_smooth_quant": True,
    },
    "gpt_350m_sq_per_token_channel": {
        "num_layers": 24,
        "num_heads": 16,
        "hidden_size": 1024,
        "vocab_size": 51200,
        "hidden_act": 'gelu',
        "n_positions": 1024,
        "max_batch_size": 256,
        "max_input_len": 512,
        "max_output_len": 200,
        "builder_opt": None,
        "use_smooth_quant": True,
        "per_token": True,
        "per_channel": True,
    },
    "opt_350m": {
        "num_layers": 24,
        "num_heads": 16,
        "hidden_size": 1024,
        "vocab_size": 50272,
        "hidden_act": 'relu',
        "n_positions": 2048,
        "max_batch_size": 256,
        "max_input_len": 512,
        "max_output_len": 200,
        "builder_opt": None,
        "pre_norm": False,
        "do_layer_norm_before": False,
    },
    "opt_66b": {
        "num_layers": 64,
        "num_heads": 72,
        "hidden_size": 9216,
        "vocab_size": 50272,
        "hidden_act": 'relu',
        "n_positions": 2048,
        "max_batch_size": 64,
        "max_input_len": 512,
        "max_output_len": 200,
        "builder_opt": None,
        "pre_norm": True,
        "do_layer_norm_before": True,
    },
    "llama_7b": {
        "num_layers": 32,
        "num_heads": 32,
        "hidden_size": 4096,
        "vocab_size": 32000,
        "hidden_act": 'silu',
        "n_positions": 2048,
        "inter_size": 11008,
        "max_batch_size": 128,
        "max_input_len": 512,
        "max_output_len": 200,
        "builder_opt": None,
    },
    "llama_30b": {
        "num_layers": 60,
        "num_heads": 52,
        "hidden_size": 6656,
        "vocab_size": 32000,
        "hidden_act": 'silu',
        "n_positions": 2048,
        "inter_size": 17920,
        "max_batch_size": 64,
        "max_input_len": 512,
        "max_output_len": 200,
        "builder_opt": None,
    },
    "gptj_6b": {
        "num_layers": 28,
        "num_heads": 16,
        "hidden_size": 4096,
        "vocab_size": 50400,
        "hidden_act": 'gelu',
        "n_positions": 1024,
        "rotary_dim": 64,
        "max_batch_size": 256,
        "max_input_len": 512,
        "max_output_len": 200,
        "builder_opt": None,
    },
    "gptneox_20b": {
        "num_layers": 44,
        "num_heads": 64,
        "hidden_size": 6144,
        "vocab_size": 50432,
        "hidden_act": 'gelu',
        "n_positions": 2048,
        "rotary_dim": 24,
        "max_batch_size": 16,
        "max_input_len": 512,
        "max_output_len": 512,
        "builder_opt": None,
    },
    "chatglm_6b": {
        "num_layers": 28,
        "num_heads": 32,
        "hidden_size": 4096,
        "vocab_size": 130528,
        "hidden_act": 'gelu',
        "n_positions": 2048,
        "max_batch_size": 256,
        "max_input_len": 512,
        "max_output_len": 200,
        "builder_opt": None,
    },
    "bert_base": {
        "num_layers": 12,
        "num_heads": 12,
        "hidden_size": 768,
        "vocab_size": 30522,
        "type_vocab_size": 2,
        "hidden_act": 'gelu',
        "n_positions": 1024,
        "max_batch_size": 256,
        "max_input_len": 512,
        "builder_opt": None,
        "enable_qk_half_accum": False,
        "enable_context_fmha": False,
    },
    "bert_large": {
        "num_layers": 24,
        "num_heads": 16,
        "hidden_size": 1024,
        "vocab_size": 30522,
        "type_vocab_size": 2,
        "hidden_act": 'gelu',
        "n_positions": 1024,
        "max_batch_size": 64,
        "max_input_len": 512,
        "builder_opt": None,
        "enable_qk_half_accum": False,
        "enable_context_fmha": False,
    },
}


def get_model_config(model_name):
    if model_name in _allowed_configs.keys():
        return _allowed_configs[model_name]
    else:
        raise Exception(f'Unexpected model: {model_name}')
