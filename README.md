<!-- <img src="docs/logo.png" align="right" alt="logo" height="180"  /> -->
<img src="assets/trt2023.jpeg" align="center" alt="logo"  />

## åŸºäºTensorRT-LLMçš„LLaMaæ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ :zap:
### LLaMa: Open and Efficient Foundation Language Models for TensorRT Hackathon 2023 <img src="assets/llama.png" alt="logo"  width=4%/>

[![](https://img.shields.io/badge/Github-TensorRT-blue)](https://github.com/NVIDIA/TensorRT)
[![](https://img.shields.io/badge/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0-TensorRT%20Hackathon%202023-blue)](https://tianchi.aliyun.com/competition/entrance/532108/introduction?spm=a2c22.12281957.0.0.4c885d9bOexwJc)
[![](https://img.shields.io/badge/NVIDIA-TensorRT%20CookBook%20CN-blue)](https://github.com/NVIDIA/trt-samples-for-hackathon-cn)
[![](https://img.shields.io/badge/B%E7%AB%99-GodV%20TensorRT%E6%95%99%E7%A8%8B-blue)](https://www.bilibili.com/video/BV1jj411Z7wG/?spm=a2c22.12281978.0.0.49ed2274CQCrY7)
[![](https://img.shields.io/badge/Github-%20%E5%88%9D%E8%B5%9B%E6%80%BB%E7%BB%93-blue)](https://github.com/TRT2022/ControlNet_TensorRT)
[![](https://img.shields.io/badge/Github-LLaMa-blue)](https://github.com/facebookresearch/llama)


:alien: : **ç¾è¿ªåº·-åŒ—èˆªAI Lab** 

### 0.â³æ—¥å¿—

<div align=center>

|æ—¶é—´ç‚¹|æäº¤å†…å®¹|è¯´æ˜|
|-|-|-|
|2023-08-21|å’ŒNVIDIAå¯¼å¸ˆå›¢é˜Ÿç¡®å®šä¼˜åŒ–æ–¹æ¡ˆï¼šåŸºäºå¼€æºLLMçš„LLaMaæ¨¡å‹æ¨æ–­åŠ é€Ÿä¼˜åŒ–|é€‰é¢˜|
|2023-08-22|åˆ›å»ºGithubé¡¹ç›®                                              |é¡¹ç›®åˆ›å»º|
|2023-08-24|å®Œæˆé€åˆ†é¢˜ä½œç­”                                               |é€åˆ†é¢˜ |


â˜£ï¸å¤èµ›è°ƒä¼˜é˜¶æ®µï¼š2023å¹´8æœˆ17æ—¥-9æœˆ21æ—¥
</div>


### 1.æ€»è¿°
---

ToDo


### 2.ä¸»è¦å¼€å‘å·¥ä½œ
---

#### 2.1 å¼€å‘å·¥ä½œçš„éš¾ç‚¹

ToDo

#### 2.2 å¼€å‘ä¸ä¼˜åŒ–è¿‡ç¨‹

ToDo

### 3.ä¼˜åŒ–æ•ˆæœ
---

ToDo

### 4.BugæŠ¥å‘Š
---

<div align=center>

|:bug: Bugåç§°|Issue|æ˜¯å¦è¢«å®˜æ–¹ç¡®è®¤|è¯´æ˜|
|-|-|:-:|-|
|InstanceNormalization Plugin |<https://github.com/NVIDIA/TensorRT/issues/3165>|â|å®˜æ–¹æš‚æœªç¡®è®¤|


</div>

### 5.é€åˆ†é¢˜ç­”æ¡ˆ
---

> ğŸ” é—®é¢˜1ï¼š è¯·å†™å‡º `./tensorrt_llm_july-release-v1/examples/gpt/README` é‡Œé¢ `â€œSingle node, single GPUâ€` éƒ¨åˆ†å¦‚ä¸‹å‘½ä»¤çš„è¾“å‡ºï¼ˆ10åˆ†ï¼‰[æ¨¡å‹ä¸ºgpt2-medium](https://huggingface.co/gpt2-medium) 

```shell
python3 run.py --max_output_len=8 
```
<details>
<summary>ğŸ”‘ç‚¹æˆ‘æŸ¥çœ‹ é—®é¢˜1 è§£æ</summary>

0. å¿…è¦çš„Python Packageå®‰è£…

```shell
cd ./tensorrt_llm_july-release-v1/examples/gpt
pip3 install requirements.txt -i https://mirrors.aliyun.com/pypi/simple
```

1. ä¸‹è½½HuggingFace(HF)æ¨¡å‹

```shell
# ä¸‹è½½HFæ¨¡å‹
rm -rf gpt2 && git clone https://huggingface.co/gpt2-medium gpt2

# æ›´æ–°.binæ¨¡å‹
cd gpt2
rm pytorch_model.bin model.safetensors
wget https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin
cd ..
```
2. å°†HF weightè½¬ä¸ºFT weight

TensorRT-LLM å¯ä»¥ç›´æ¥åŠ è½½FastTransformer(FT)æ ¼å¼çš„æ¨¡å‹weightæ–‡ä»¶ï¼Œå› æ­¤éœ€è¦å°†HF weightè½¬æ¢ä¸ºFT weight

```shell
python3 hf_gpt_convert.py -i gpt2 -o ./c-model/gpt2 --tensor-parallelism 1 --storage-type float16
```
è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹è½¬æ¢å®Œæˆï¼Œå¹¶åœ¨`tensorrt_llm_july-release-v1/examples/gpt/c-model/gpt2/1-gpu`ä¸­å­˜æ”¾äº†ç”Ÿæˆåçš„FT weight

<div align=center>
<img src="./assets/section5/p1.png"/>
</div>

3. æ„å»ºTensorRT-LLM engine

TensorRT-LLM engineçš„æ„å»ºè¿‡ç¨‹ä½¿ç”¨äº†FT weightå’Œå¯¹åº”çš„é…ç½®æ–‡ä»¶ï¼ˆå·²ç»åœ¨ç¬¬2æ­¥ç”Ÿæˆï¼‰å’Œè‡ªå®šä¹‰çš„Tokenizerã€‚è¿‡ç¨‹ä¸­å¦‚æœä¸æŒ‡å®šæ¨¡å‹æƒé‡è·¯å¾„ï¼ŒTensorRT-LLMé»˜è®¤éšæœºåˆå§‹åŒ–è¿™äº›weightç”Ÿæˆengineã€‚

```shell
# single GPU float16 ä½¿ç”¨FT weightç”Ÿæˆengine
python3 build.py --model_dir=./c-model/gpt2/1-gpu --use_gpt_attention_plugin
```
æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹åºåˆ—åŒ–å®Œæˆï¼Œå¹¶å°†engineä¿å­˜åœ¨`./tensorrt_llm_july-release-v1/examples/gpt/gpt_outputs`ä¸­

```
[08/24/2023-07:07:38] [TRT-LLM] [I] Total time of building gpt_float16_tp1_rank0.engine: 00:00:35
[08/24/2023-07:07:38] [TRT-LLM] [I] Config saved to gpt_outputs/config.json.
[08/24/2023-07:07:38] [TRT-LLM] [I] Serializing engine to gpt_outputs/gpt_float16_tp1_rank0.engine...
[08/24/2023-07:07:42] [TRT-LLM] [I] Engine serialized. Total time: 00:00:04
[08/24/2023-07:07:42] [TRT-LLM] [I] Timing cache serialized to gpt_outputs/model.cache
[08/24/2023-07:07:42] [TRT-LLM] [I] Total time of building all 1 engines: 00:00:50
```
âš ï¸æ³¨æ„ï¼š `build.py`æ”¯æŒå¤šGPUå¹¶è¡Œæ„å»ºTensorRT-LLM engine,è¯¦ç»†çš„å¯ä»¥å‚è€ƒï¼š`./tensorrt_llm_july-release-v1/examples/gpt/README`

4. Single node,single GPUä¸‹æµ‹è¯•engine

```shell
# single GPUä¸‹è¿è¡Œengine
python3 run.py --max_output_len=8
```
è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
Input: Born in north-east France, Soyer trained as a
Output:  chef and eventually became a chef at a
```
é—®é¢˜1è§£æå®Œæˆã€‚

âš ï¸æ³¨æ„ï¼š`run.py`æ”¯æŒsingle node multiple GPUs(åŸºäº`mpirun`)å’Œmultiple nodes,multiple GPUs(åŸºäº[Slurm](https://slurm.schedmd.com))ï¼Œè¯¦ç»†çš„å¯ä»¥å‚è€ƒ`./tensorrt_llm_july-release-v1/examples/gpt/README`

</details>


> ğŸ” é—®é¢˜2: è¯·å†™å‡º `./tensorrt_llm_july-release-v1/examples/gpt/README` é‡Œé¢ `â€œSummarization using the GPT modelâ€` éƒ¨åˆ†å¦‚ä¸‹å‘½ä»¤çš„rouge åˆ†æ•°ï¼ˆ10åˆ†ï¼‰[æ¨¡å‹ä¸ºgpt2-medium](https://huggingface.co/gpt2-medium)

```shell
python3 summarize.py --engine_dirtrt_engine/gpt2/fp16/1-gpu --test_hf  --batch_size1  --test_trt_llm  --hf_model_location=gpt2 --check_accuracy --tensorrt_llm_rouge1_threshold=14
```

<details>
<summary>ğŸ”‘ç‚¹æˆ‘æŸ¥çœ‹ é—®é¢˜2 è§£æ</summary>

è¯¥é—®é¢˜å°†æè¿°å¦‚ä½•ä½¿ç”¨TensorRT-LLMè¿è¡Œä¸€ä¸ªæ–‡æœ¬æ‘˜è¦ä»»åŠ¡çš„GPT-2,è¿™é‡Œä½¿ç”¨çš„æ•°æ®é›†ä¸º[cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) ï¼Œå¯¹åº”ç”Ÿæˆçš„æ‘˜è¦ä½¿ç”¨[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) scoreæ¥è¯„ä»·TensorRT-LLMçš„ç²¾åº¦å˜åŒ–ï¼Œç¡®åˆ‡çš„è¯´è¿™é‡Œä½¿ç”¨äº†`ROUGE-1` scoreã€‚

å…³äºè¯„ä»·æŒ‡æ ‡[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric))çš„ä»‹ç»ï¼Œæˆ‘ä»¬æ¨èå‚è€ƒçŸ¥ä¹çš„ä»‹ç»ï¼š

+ [ä¸­æ–‡æ–‡æœ¬æ‘˜è¦æŒ‡æ ‡-ROUGE](https://zhuanlan.zhihu.com/p/388720967)
+ [NLPè¯„ä¼°æŒ‡æ ‡ä¹‹ROUGE](https://zhuanlan.zhihu.com/p/504279252)

é—®é¢˜1ä¸­å·²ç»å®Œæˆäº†å¿…è¦çš„packageçš„å®‰è£…ï¼Œè¿™é‡Œç›´æ¥ä¸‹è½½HFæ¨¡å‹

1. ä¸‹è½½HFæ¨¡å‹æ–‡ä»¶

```shell
# ä¸‹è½½æ¨¡å‹æ–‡ä»¶
rm -rf gpt2 && git clone https://huggingface.co/gpt2 gpt2

# æ›´æ–°.binæ¨¡å‹æ–‡ä»¶
cd gpt2
rm pytorch_model.bin model.safetensors
wget https://huggingface.co/gpt2/resolve/main/pytorch_model.bin
cd ..
```

2. å°†HF weightè½¬æ¢ä¸ºFT weight

```shell
python3 hf_gpt_convert.py -i gpt2 -o ./c-model/gpt2/fp16 --tensor-parallelism 1 --storage-type float16
```

è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹è½¬æ¢å®Œæˆï¼Œå¹¶åœ¨`tensorrt_llm_july-release-v1/examples/gpt/c-model/gpt2/fp16/1-gpu`ä¸­å­˜æ”¾äº†ç”Ÿæˆåçš„FT weight

<div align=center>
<img src="./assets/section5/p2.png"/>
</div>

3. æ„å»ºTensorRT-LLM engine

```shell
python3 build.py --model_dir=./c-model/gpt2/fp16/1-gpu \
                 --use_gpt_attention_plugin \
                 --use_gemm_plugin \
                 --use_layernorm_plugin \
                 --max_batch_size 8 \
                 --max_input_len 924 \
                 --max_output_len 100 \
                 --output_dir trt_engine/gpt2/fp16/1-gpu/ \
                 --hidden_act gelu
```

æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹åºåˆ—åŒ–å®Œæˆï¼Œå¹¶å°†engineä¿å­˜åœ¨`./tensorrt_llm_july-release-v1/examples/gpt/trt_engine/gpt2/fp16/1-gpu`ä¸­

```
[08/24/2023-07:34:42] [TRT-LLM] [I] Total time of building gpt_float16_tp1_rank0.engine: 00:01:32
[08/24/2023-07:34:42] [TRT-LLM] [I] Config saved to trt_engine/gpt2/fp16/1-gpu/config.json.
[08/24/2023-07:34:42] [TRT-LLM] [I] Serializing engine to trt_engine/gpt2/fp16/1-gpu/gpt_float16_tp1_rank0.engine...
[08/24/2023-07:34:43] [TRT-LLM] [I] Engine serialized. Total time: 00:00:01
[08/24/2023-07:34:43] [TRT-LLM] [I] Timing cache serialized to trt_engine/gpt2/fp16/1-gpu/model.cache
[08/24/2023-07:34:43] [TRT-LLM] [I] Total time of building all 1 engines: 00:01:43
```

4. TensorRT-LLMä¸‹æµ‹è¯•GPT-2æ–‡æœ¬æ‘˜è¦ä»»åŠ¡

```shell
python3 summarize.py --engine_dir trt_engine/gpt2/fp16/1-gpu \
                     --test_hf \
                     --batch_size 1 \
                     --test_trt_llm \
                     --hf_model_location=gpt2 \
                     --check_accuracy \
                     --tensorrt_llm_rouge1_threshold=14
```
æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œç»“æœå¦‚ä¸‹ï¼š

```
[08/24/2023-08:40:54] [TRT-LLM] [I] ---------------------------------------------------------
Downloading builder script: 5.60kB [00:00, 6.22MB/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1151 > 1024). Running this sequence through the model will result in indexing errors
[08/24/2023-08:41:14] [TRT-LLM] [I] TensorRT-LLM (total latency: 2.6481666564941406 sec)
[08/24/2023-08:41:14] [TRT-LLM] [I] TensorRT-LLM beam 0 result
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge1 : 15.361040799540035
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge2 : 3.854022269668396
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeL : 12.078455591738333
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeLsum : 13.547802733617264
[08/24/2023-08:41:14] [TRT-LLM] [I] Hugging Face (total latency: 10.39808702468872 sec)
[08/24/2023-08:41:14] [TRT-LLM] [I] HF beam 0 result
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge1 : 14.75593024343394
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge2 : 3.3647470801871733
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeL : 11.124766996533
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeLsum : 13.031128048110618
```

<div align=center>
<img src="./assets/section5/p3.png"/>
</div>

é—®é¢˜2è§£æå®Œæˆã€‚

âš ï¸æ³¨æ„ï¼šè¿‡ç¨‹ä¸­éœ€è¦ä¸‹è½½æ•°æ®ï¼Œå¦‚æœè¿è¡Œè¿‡ç¨‹ä¸­ä¸­æ–­æˆ–æŠ¥é”™å¤§å¤šæ˜¯å› ä¸ºç½‘ç»œåŸå› ï¼Œè¯·è€å¿ƒå¤šå°è¯•é‡å¤è¿è¡Œå‡ æ¬¡

</details>


### 6.æœªæ¥å·¥ä½œ
---

ToDo

### 7.ğŸ˜‰References
---

1. [TensorRT(Github)](https://github.com/NVIDIA/TensorRT)

2. [trt-samples-for-hackathon-cn](https://github.com/NVIDIA/trt-samples-for-hackathon-cn)

3. [NVIDIA TensorRT Hackathon 2023 â€”â€” ç”Ÿæˆå¼AIæ¨¡å‹ä¼˜åŒ–èµ›(å¤©æ± )](https://tianchi.aliyun.com/competition/entrance/532108/introduction?spm=a2c22.12281957.0.0.605a3b74bkLhBT)

4. [TensorRT 8.6 è®²åº§(Bç«™)](https://www.bilibili.com/video/BV1jj411Z7wG/)

5. [Llama(Github)](https://github.com/facebookresearch/llama)

6. [Llama paper (arxiv)](https://arxiv.org/abs/2302.13971)

7. [TensorRT-LLM:å¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼šä½ç²¾åº¦æœ€ä½³å®è·µ(Bç«™)](https://www.bilibili.com/video/BV1h44y1c72B/?share_source=copy_web&vd_source=db3eecb1b88cc6c7a18eeaf6db1ed114)


