<!-- <img src="docs/logo.png" align="right" alt="logo" height="180"  /> -->
<img src="assets/trt2023.jpeg" align="center" alt="logo"  />

## åŸºäºTensorRT-LLMçš„LLaMAæ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ :zap:
### LLaMA: Open and Efficient Foundation Language Models for TensorRT Hackathon 2023 <img src="assets/llama.png" alt="logo"  width=4%/>

[![](https://img.shields.io/badge/Github-TensorRT-blue)](https://github.com/NVIDIA/TensorRT)
[![](https://img.shields.io/badge/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0-TensorRT%20Hackathon%202023-blue)](https://tianchi.aliyun.com/competition/entrance/532108/introduction?spm=a2c22.12281957.0.0.4c885d9bOexwJc)
[![](https://img.shields.io/badge/NVIDIA-TensorRT%20CookBook%20CN-blue)](https://github.com/NVIDIA/trt-samples-for-hackathon-cn)
[![](https://img.shields.io/badge/B%E7%AB%99-GodV%20TensorRT%E6%95%99%E7%A8%8B-blue)](https://www.bilibili.com/video/BV1jj411Z7wG/?spm=a2c22.12281978.0.0.49ed2274CQCrY7)
[![](https://img.shields.io/badge/Github-%20%E5%88%9D%E8%B5%9B%E6%80%BB%E7%BB%93-blue)](https://github.com/TRT2022/ControlNet_TensorRT)
[![](https://img.shields.io/badge/Github-LLaMA-blue)](https://github.com/facebookresearch/llama)


:alien: : **ç¾è¿ªåº·-åŒ—èˆªAI Lab** 

### 0.â³æ—¥å¿—

<div align=center>

|æ—¶é—´ç‚¹|æäº¤å†…å®¹|è¯´æ˜|
|-|-|-|
|2023-08-21|å’ŒNVIDIAå¯¼å¸ˆå›¢é˜Ÿç¡®å®šä¼˜åŒ–æ–¹æ¡ˆï¼šåŸºäºå¼€æºLLMçš„LLaMAæ¨¡å‹æ¨æ–­åŠ é€Ÿä¼˜åŒ–|é€‰é¢˜|
|2023-08-22|åˆ›å»ºGithubé¡¹ç›®                                              |é¡¹ç›®åˆ›å»º|
|2023-08-24|å®Œæˆé€åˆ†é¢˜ä½œç­”                                               |é€åˆ†é¢˜ |
|2023-08-31|examples/llamaæºç å­¦ä¹                                        |é¡¹ç›®åˆ†æ |
|2023-09-3|æ­£å¸¸è¿è¡Œexamples/llama                                     |ä»£ç è¿è¡Œæµ‹è¯• |


â˜£ï¸å¤èµ›è°ƒä¼˜é˜¶æ®µï¼š2023å¹´8æœˆ17æ—¥-9æœˆ21æ—¥
</div>


### 1.æ€»è¿°
---

ä½œä¸º [NVIDIA TensorRT Hackathon 2023](https://github.com/NVIDIA/trt-samples-for-hackathon-cn/tree/master/Hackathon2023) çš„å¤èµ›å‚èµ›é¢˜ç›®ï¼Œæœ¬å·¥ä½œåŸºäºTensorRT-LLMä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹LLaMAã€‚æŒ‰ç…§å‚èµ›è¦æ±‚æœ¬é¡¹ç›®é€‰æ‹©å®Œæˆ**TensorRT-LLMè¯•ç”¨é€åˆ†é¢˜**ä»¥åŠ**3+4æ¨¡å¼çš„TensorRT-LLMæ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ**ï¼Œå³ï¼š
+ 3ï¼šç”¨TensorRT-LLMä¼˜åŒ–examplesç›®å½•ä¸‹çš„æŸä¸ªç°æœ‰æ¨¡å‹ï¼ˆæœ¬é¡¹ç›®ä¼˜åŒ–`examples/llama`æ¨¡å‹ï¼‰
+ 4ï¼šå°è¯•ä¸ºTensorRT-LLMæ·»åŠ æ–°featureï¼Œæˆ–è€…åœ¨æ¨¡å‹ä¸Šå¯ç”¨äº†ç°æœ‰feature

ä¸‹é¢æˆ‘ä»¬å°†ä»æ¨¡å‹ä»‹ç»ï¼Œä¼˜åŒ–æ•ˆæœåŠå¦‚ä½•è¿è¡Œè¯¥é¡¹ç›®ç­‰3ä¸ªæ–¹é¢ä»‹ç»æœ¬é¡¹ç›®

é¦–å…ˆæ˜¯LLaMAæ¨¡å‹çš„ç›¸å…³ä»‹ç»ï¼Œ[LLaMA](https://github.com/facebookresearch/llama) æ˜¯ç›®å‰ä¸ºæ­¢ï¼Œæ•ˆæœæœ€å¥½çš„å¼€æº LLM ä¹‹ä¸€,æ•°æ®é›†å±‚é¢ä¸Šå…±æœ‰1.4Tçš„Tokens, tokenizerä½¿ç”¨byte pair encoding (BPE) ç®—æ³•ï¼ŒSentence-Pieceçš„å®ç°,æ‰€æœ‰æ•°å­—è¢«æ‹†åˆ†ä¸ºå•ç‹¬çš„digitï¼Œæ‰€æœ‰æœªçŸ¥çš„UTF-8 å­—ç¬¦ï¼Œå›é€€åˆ°å­—èŠ‚æ¥è¿›è¡Œåˆ†è§£ã€‚å› æ­¤ï¼ŒLLaMA å¯ä»¥é€šè¿‡byte çš„æ–¹å¼ï¼Œæ„é€ å‡ºå¾ˆå¤šä¸åœ¨ vocab ä¸­çš„å­—ç¬¦ï¼Œä»è€Œä¹Ÿå…·æœ‰è¾ƒå¥½çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚ç½‘ç»œç»“æ„ä¸Šçš„æ”¹è¿›åŸºäºTransformerçš„æ¶æ„ï¼Œå¹¶åšäº†å¦‚ä¸‹3ç‚¹æ”¹è¿›ï¼š
+ Pre-Normalizationï¼šä¸ºäº†æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¯¹æ¯ä¸ªtransformerå±‚çš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ï¼ˆä½¿ç”¨ RMS Norm å½’ä¸€åŒ–å‡½æ•°ï¼‰
+ SwiGLUï¼šä½¿ç”¨SwiGLUæ›¿ä»£äº†ReLUä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚å’ŒPaLMä¸­ä¸åŒï¼Œç»´åº¦é‡‡ç”¨$\frac{2}{3}4d$è€Œä¸æ˜¯$4d$  
+ RoPEï¼šé‡‡ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼Œä½¿å¾—å¤§æ¨¡å‹çš„ç”Ÿæˆæœ‰æ›´å¥½çš„å¤–æ¨æ€§

LLaMA-7Bæœ‰32ä¸ªè¿™æ ·çš„transformer blockæ„æˆï¼ŒLLaMA-13B ä¼˜äº GPT-3ï¼Œå°½ç®¡åªæœ‰1/10å¤§å°ã€‚ LLaMA-65B æ˜¯å¯ä»¥ä¸ Chinchilla-70B å’Œ PaLM-540B è¿™ç§æœ€ä½³çš„LLMç›¸ç«äº‰çš„æ¨¡å‹ã€‚ç»è¿‡å¾®è°ƒä¹‹åï¼ŒLLaMAçš„æ•ˆæœæœ‰æ˜¾è‘—çš„æå‡ã€‚å…³äºLLaMAçš„ä»‹ç»ï¼Œæ¨èçŸ¥ä¹æ–‡ç« ï¼š
+ [LLaMA è¶…è¯¦ç»†è§£è¯»ï¼ˆpaper & codeï¼‰](https://zhuanlan.zhihu.com/p/632102048?utm_id=0)

é’ˆå¯¹äºLLaMA-7Bå’ŒTensorRT-LLMä¸‹çš„`examples/llama`,æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹æ¡ˆè®¡åˆ’å®ç°è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<div align=center>
<img src="./assets/TensorRT-LLM LLaMaè¿›åº¦å›¾.png"/>
</div>

ToDoï¼š
- ä¼˜åŒ–æ•ˆæœï¼ˆä¾‹å¦‚ç»™å‡ºç²¾åº¦å’ŒåŠ é€Ÿæ¯”ï¼‰ï¼Œç®€å•ç»™å‡ºå…³é”®çš„æ•°å­—å³å¯ï¼Œåœ¨è¿™é‡Œä¸å¿…è¯¦ç»†å±•å¼€
- åœ¨Dockeré‡Œé¢ä»£ç ç¼–è¯‘ã€è¿è¡Œæ­¥éª¤çš„å®Œæ•´è¯´æ˜
  - è¯·åšåˆ°åªè¦é€è¡Œè¿è¡Œä½ ç»™çš„å‘½ä»¤ï¼Œå°±èƒ½æŠŠä»£ç è·‘èµ·æ¥


### 2.ä¸»è¦å¼€å‘å·¥ä½œ
---

#### 2.1 å¼€å‘å·¥ä½œçš„éš¾ç‚¹

è¯¥æ¨¡å‹çš„ä¼˜åŒ–éš¾ç‚¹å¦‚ä¸‹ï¼š

+ å¯¹äºTensorRT-LLMç›¸å…³åŠŸèƒ½å’ŒAPIä¸ç†Ÿæ‚‰
+ `examples/llama`ä¸­å·²ç»å®ç°äº†å¤šæ•°çš„featureï¼Œåœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥çš„ä¼˜åŒ–éš¾åº¦è¾ƒå¤§

ä½†æ˜¯ï¼ŒLLaMAä½œä¸ºç›¸å¯¹è¾ƒæ—©çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œå…¶ç›´æ¥å½±å“äº†å›½å†…å¤–å¤§æ¨¡å‹çš„å‘å±•å’Œç ”å‘æ€è·¯ï¼Œå…¶é‡è¦æ€§ä¸è¨€è€Œå–»ï¼Œé’ˆå¯¹äºLLaMAçš„TensorRT-LLMæ¨¡å‹ä¼˜åŒ–æ„ä¹‰é‡å¤§ã€‚

#### 2.2 å¼€å‘ä¸ä¼˜åŒ–è¿‡ç¨‹

é’ˆå¯¹äºLLaMA-7Bæˆ‘ä»¬çš„ä¼˜åŒ–è¿‡ç¨‹ä¸»è¦åˆ†ä»¥ä¸‹3ä¸ªéƒ¨åˆ†ï¼š
+ 1.åˆæ­¥è¿è¡Œ`examples/llama`é¡¹ç›®
+ 2.nsight systermåˆ†æé€æ­¥æ·»åŠ featureè¿›è¡Œæ¶ˆèå®éªŒ
+ 3.æ–°featuteå®ç°ï¼šint8 k/v cache, smoothquantå’Œinflight batching

æ¯ä¸€éƒ¨åˆ†æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„è¿è¡Œè„šæœ¬å’Œæµ‹è¯•ç»“æœã€‚

##### 2.2.1 åˆæ­¥è¿è¡Œ`examples/llama`é¡¹ç›®

0. å‡†å¤‡LLaMA-7B meta checkpointæ¨¡å‹

+ ä¸‹è½½æ¨¡å‹

LlaMA-7B v1 (meta checkpoint)æ¨¡å‹ä¸‹è½½åœ°å€ï¼š <https://115.com/s/sw6a2kv3w4z?password=a835&#>,å°†ä¸‹è½½åçš„æ¨¡å‹å­˜æ”¾åœ¨`/tensorrt_llm_july-release-v1/examples/llama/llama-1-7b-meta/`ä¸‹

+ meta checkpoint è½¬ huggingface(HF) checkpoint

```shell
# cdåˆ°ç›®æ ‡è·¯å¾„
cd ./tensorrt_llm_july-release-v1/examples/llama
# æ¨¡å‹è½¬HF checkpoint
python3 /usr/local/lib/python3.8/dist-packages/transformers/models/llama/convert_llama_weights_to_hf.py  --input_dir ./llama-1-7b-meta --model_size 7B --output_dir ./tmp/llama/7B
```

1. æ„å»ºTensorRT-LLM engine

```shell
# ä¸åŠ å…¥ä»»ä½•trick
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu \
                --visualize

# åŠ å…¥plugin
python build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu/
```

2. è¿è¡Œengine

```shell
python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/
```

3. ä½¿ç”¨LLaMA-7Bæµ‹è¯•æ–‡æœ¬æ‘˜è¦ä»»åŠ¡

```shell
# ä½¿ç”¨TensorRT-LLM engineæµ‹è¯•
python3 summarize.py --test_trt_llm \
                    --hf_model_location ./tmp/llama/7B/ \
                    --data_type fp16 \
                    --engine_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu/
```
ç»“æœï¼š

```
[09/03/2023-13:56:21] [TRT-LLM] [I] ---------------------------------------------------------
[09/03/2023-13:57:32] [TRT-LLM] [I] TensorRT-LLM (total latency: 65.88509821891785 sec)
[09/03/2023-13:57:32] [TRT-LLM] [I] TensorRT-LLM beam 0 result
[09/03/2023-13:57:32] [TRT-LLM] [I]   rouge1 : 19.478572394974464
[09/03/2023-13:57:32] [TRT-LLM] [I]   rouge2 : 5.748473587185184
[09/03/2023-13:57:32] [TRT-LLM] [I]   rougeL : 14.488586709461371
[09/03/2023-13:57:32] [TRT-LLM] [I]   rougeLsum : 17.818188740969955
```

```shell
# ä½¿ç”¨HFæ¨¡å‹æµ‹è¯•
python3 summarize.py --test_hf \
                    --hf_model_location ./tmp/llama/7B/ \
                    --data_type fp16 
```

ç»“æœï¼š

```
[09/03/2023-14:02:07] [TRT-LLM] [I] ---------------------------------------------------------
[09/03/2023-14:03:30] [TRT-LLM] [I] Hugging Face (total latency: 78.22841620445251 sec)
[09/03/2023-14:03:30] [TRT-LLM] [I] HF beam 0 result
[09/03/2023-14:03:30] [TRT-LLM] [I]   rouge1 : 20.106338916310662
[09/03/2023-14:03:30] [TRT-LLM] [I]   rouge2 : 5.910110463256421
[09/03/2023-14:03:30] [TRT-LLM] [I]   rougeL : 15.2269090887293
[09/03/2023-14:03:30] [TRT-LLM] [I]   rougeLsum : 17.938095329383458
```

æˆ‘ä»¬åˆæ­¥æ­£å¸¸è¿è¡Œäº†`example/llama`,ä»ç»“æœä¸Šåˆæ­¥éªŒè¯äº†æ­£ç¡®æ€§ã€‚

##### 2.2.2 nsight systemåˆ†æé€æ­¥æ·»åŠ featureè¿›è¡Œæ¶ˆèå®éªŒ

è¯¥éƒ¨åˆ†æˆ‘ä»¬åšäº†è¯¦ç»†çš„æ¶ˆèå®éªŒï¼Œé€šè¿‡é€æ­¥æ·»åŠ featureå’Œtrickçš„æ–¹å¼éªŒè¯ä¸åŒfeatureåœ¨LLaMA-7Bä¸Šçš„Latencyçš„æ”¶ç›Šï¼Œå¹¶åŸºäºnsight systemè¿›è¡Œprofilingã€‚

ç›®å‰`examples/llama`çš„featureæ”¯æŒæƒ…å†µå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<div align=center>
<img src="./assets/TensorRT_LLM LLaMa.png"/>
</div>

ç”±äºLLaMA-7Bä¸­ä½¿ç”¨äº†RoPE,ç›®å‰`gpt_attention_plugin`æ˜¯å”¯ä¸€çš„ä¸€ç§æ”¯æŒRoPEçš„æ–¹å¼ï¼Œå› æ­¤LLaMAåœ¨TensorRT-LLMä¸­å¼ºåˆ¶ä½¿ç”¨äº†`gpt_attention_plugin`

1. æ·»åŠ : k/v cache + attention pligin

+ build engine

```shell
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu \
```

+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
# è¿è¡Œengine
nsys profile -o trt_llm_only_kv_cache_fp16 python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/

# è¿è¡ŒHF checkpoint
python3 run_hf.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --hf_model_location ./tmp/llama/7B/
```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 1.404158673286438 sec)
# HF
llama-hf-run (mean latency: 1.7185083055496215 sec)
```
ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `k/v cache + attention plugin`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.40416ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`1.224`

åˆ†æå¯¼å‡ºçš„`trt_llm_only_kv_cache_fp16` nsysæ–‡ä»¶ï¼Œå¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°attention pluginå’Œk/v cacheä»¥åŠçŸ©é˜µä¹˜çš„æ¨ç†å»¶æ—¶æƒ…å†µå¦‚ä¸‹æ‰€ç¤ºï¼š

+ attention plugin profilingçš„è€—æ—¶æƒ…å†µ
<div align=center>
<img src="./assets/kv_cache_fp16/attention_1.png"/>
</div>

+ k/v cache profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/kv_cache_fp16/kvcache.png"/>
</div>

+ å¸¦æƒé‡çš„çŸ©é˜µä¹˜çš„profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/kv_cache_fp16/matrix_multiply_0.png"/>
</div>

å¯ä»¥çœ‹åˆ°åœ¨FP16ä¸‹ï¼Œattention pluginçš„latencyä¸º$13.576\mu s$,k/v cacheçš„latencyä¸º$925.928\mu s$,å¸¦æƒé‡çš„çŸ©é˜µä¹˜çš„latencyä¸º$45.922\mu s$

2. æ·»åŠ : k/v cache + attention_plugin + weight_only_quant

+ build engine

```shell
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_weight_only \
                --output_dir ./tmp/llama/7B/trt_engines/int8_kvcache/1-gpu/
```

+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
nsys profile -o trt_llm_weight_only python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/weight_only/1-gpu/
```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 0.7849386262893677 sec)
```
ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `k/v cache + attention plugin + weight_only_quant`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`0.78494ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`2.189`

åˆ†æ`trt_llm_weight_only`nsysæ–‡ä»¶ï¼Œå¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°attention pluginå’ŒçŸ©é˜µä¹˜çš„æ¨ç†å»¶æ—¶æƒ…å†µå¦‚ä¸‹æ‰€ç¤ºï¼š

+ attention plugin profilingçš„è€—æ—¶æƒ…å†µ
<div align=center>
<img src="./assets/weight_only_quant/attention.png"/>
</div>

+ å¸¦æƒé‡çš„çŸ©é˜µä¹˜çš„profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/weight_only_quant/matmul.png"/>
</div>

å¯ä»¥çœ‹åˆ°åœ¨weight only quantä¸‹ï¼Œattention pluginçš„latencyä¸º$17.837\mu s$,å¸¦æƒé‡çš„çŸ©é˜µä¹˜çš„latencyä¸º$7.107\mu s$ã€‚å¯¹æ¯”ä¸Šè¿°FP16çš„æƒ…å†µæœ‰æ˜æ˜¾çš„åŠ é€Ÿæ•ˆæœã€‚


3. æ·»åŠ : k/v cache + attention plugin + weight_only_quant + gemm plugin

+ build engine

```shell
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --use_weight_only \
                --output_dir ./tmp/llama/7B/trt_engines/weight_only_attention_gemm/1-gpu/

```
+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
nsys profile -o trt_llm_weight_only_attention_gemm python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/weight_only_attention_gemm/1-gpu/

```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 0.7930449199676514 sec)
```
ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `k/v cache + attention plugin + weight_only_quant + gemm plugin`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`0.79304ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`2.167`

åˆ†æ`trt_llm_weight_only_attention_gemm`nsysæ–‡ä»¶ï¼Œå¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°gemmåœ¨ä½¿ç”¨pluginå‰åçš„æ¨ç†å»¶æ—¶æƒ…å†µå¦‚ä¸‹æ‰€ç¤ºï¼š

+ gemm pluginå‰ profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/gemm/gemm_origin_5.png"/>
</div>

+  gemm pluginå‰ profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/gemm/gemm_5.png"/>
</div>

å¯ä»¥æ˜æ˜¾çœ‹åˆ°æ›¿æ¢gemm pluginçš„å‰åå˜åŒ–ï¼Œgemm pluginæ›¿æ¢å‰çš„latencyä¸º$28.286\mu s$,gemm pluginæ›¿æ¢åçš„latencyä¸º$26.241\mu s$,æœ‰ä¸€å®šçš„åŠ é€Ÿæ•ˆæœã€‚

4. int4 weight only quant

+ build engine

```shell
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --use_weight_only \
                --weight_only_precision int4 \
                --output_dir ./tmp/llama/7B/trt_engines/int4/1-gpu/

```
+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
nsys profile -o trt_llm__int4 python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/int4/1-gpu/
```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 0.48769086837768555 sec)
```

ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `int4`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`0.48769ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`3.524`

åˆ†æ`trt_llm__int4`nsysæ–‡ä»¶ï¼Œå¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°attention pluginå’Œk/v cacheæ¨ç†å»¶æ—¶æƒ…å†µå¦‚ä¸‹æ‰€ç¤ºï¼š

+ attention plugin profilingçš„è€—æ—¶æƒ…å†µ
<div align=center>
<img src="./assets/int4/attention.png"/>
</div>

+ k/v cache profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/int4/kvcache.png"/>
</div>

å¯ä»¥çœ‹åˆ°åœ¨int4ä¸‹ï¼Œattention pluginçš„latencyä¸º$11.609\mu s$,K/V cacheçš„latencyä¸º$159.717\mu s$

ç»¼ä¸ŠåŸºäºä¸Šè¿°åˆ†æç»“æœï¼Œæ€»ç»“å¦‚ä¸‹ï¼š
<div align=center>

|Feature|åŸLlamaæ˜¯å¦å®ç°|æœ¬é¡¹ç›®æ˜¯å¦å¯ç”¨|batch size|input length|output length|åŠ é€Ÿæ¯”|
|-|-|-|-|-|-|-|
| K/V cache|âœ”ï¸|âœ”ï¸|1|8|50|-|
|+Attention Plugin|âœ”ï¸|âœ”ï¸|1|8|50|1.224|
|+Weight Only Quant|âœ”ï¸|âœ”ï¸|1|8|50|2.189|
|+Gemm Plugin|âœ”ï¸|âœ”ï¸|1|8|50|2.167|
|+Int4 Weight Only Quant|âœ”ï¸|âœ”ï¸|1|8|50|3.524|
|+Int8 K/V cache|âŒ|-|1|8|50|-|
|SmoothQuant|âŒ|-|1|8|50|-|
|Inflight Batching|âŒ|-|1|8|50|-|

</div>

âš ï¸æ³¨æ„ï¼šæˆ‘ä»¬å°†åœ¨Section3-ä¼˜åŒ–æ•ˆæœçš„ç¬¬ä¸€éƒ¨åˆ†æä¾›ç°æœ‰featureä¸‹çš„åŠ é€Ÿæ•ˆæœå’Œç²¾åº¦å¯¹æ¯”ã€‚

#### 2.2.3 æ–°featuteå®ç°ï¼šint8 k/v cache,smoothquantï¼Œinflight batching

1.int8 k/v cache

<div align=center>
<img src="./assets/int8_kv_cache.png"/>
</div>

int8 k/v cacheæœ¬è´¨å’Œweight only quantä¸€æ ·ï¼Œåœ¨æ¨¡å‹generation phaseè¯»å–ä¹‹å‰çš„Kå’ŒVç±»ä¼¼äºweight only quantä¸­å…¨è¿æ¥å±‚ä¸­è¯»å–weight,å‚ç…§weight only quantä¹ŸæŠŠK,Vç”¨int8ä¿å­˜ä¸‹æ¥ï¼Œå½“æˆ‘ä»¬çœŸæ­£è®¡ç®—çš„æ—¶å€™å†å°†å…¶dequantåˆ°é«˜ç²¾åº¦ã€‚

`examples/llama`æš‚æ—¶ä¸æ”¯æŒint8 k/v cache,è¿™é‡Œæˆ‘ä»¬å®ç°äº†`examples/llama`çš„int8 k/v cache

ToDo:int8 k/v cacheçš„å®ç°


2.smoothquant

<div align=center>
<img src="./assets/smoothquant1.png"/>
</div>

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒæŸäº›LLMçš„æŸäº›channelæˆ–æŸäº›ç»´åº¦çš„activation outlierå€¼å¾ˆå¤šä¸”å¾ˆå¤§(ep. GLM-130Bæœ‰30%ï¼‰ï¼Œå¯¼è‡´é‡åŒ–çš„æœ‰æ•ˆä½å˜å°‘ï¼Œæ¯”å¦‚int8æœ¬æ¥æ˜¯-128åˆ°127ï¼Œæ²¡æœ‰outlierçš„æ—¶å€™ï¼Œæ˜ å°„åˆ°-128åˆ°127çš„æ•°æ®åˆ†å¸ƒå‡åŒ€ï¼Œå æ»¡äº†8bitä½èŒƒå›´ï¼Œç²¾åº¦æŸå¤±å¾ˆä½ï¼Œä½†æ˜¯æœ‰äº†outlierä¹‹åï¼Œå¤šæ•°æ­£å¸¸å€¼çš„åˆ†å¸ƒåŒºé—´å¯èƒ½åœ¨[-20,20]æˆ–è€…[-10,10]ï¼Œ8bitä½èŒƒå›´åªåˆ©ç”¨åˆ°äº†5bitï¼Œç”šè‡³4bitï¼Œç”±æ­¤å¯¼è‡´ç²¾åº¦æŸå¤±ã€‚ä¸Šå›¾ä¸­çš„activationçº¢è‰²æ©™è‰²æ˜¯outlierï¼Œoutlierä¸€èˆ¬é›†ä¸­å­˜åœ¨äºæŸå‡ ä¸ªchannelæˆ–axis,ä¸”activationæ¯”weightæ›´éš¾é‡åŒ–ï¼Œåè€…æ•°æ®åˆ†å¸ƒä¸€èˆ¬æ¯”è¾ƒå‡åŒ€ï¼Œsmoothquantçš„keypointsæ˜¯å¯ä»¥æŠŠactivationé‡åŒ–éš¾åº¦è¿ç§»åˆ°weightä¸Šæ¥ï¼ŒæŠŠactivationé‡Œé¢ä¸å‡åŒ€çš„åˆ†å¸ƒç”¨weightä¸­å’Œä¸€ä¸‹ï¼Œå…·ä½“æ¥è®²ï¼Œä¸»è¦æ˜¯åœ¨fp32é˜¶æ®µåšçš„ï¼Œä¿è¯ä¸€ä¸ªç­‰å¼çš„ç­‰ä»·ï¼Œ$X$ä¸ºè¾“å…¥activationï¼Œ$W$ä¸ºweightï¼Œ$s$ä¸ºå› å­ï¼Œé€šè¿‡$s$æ¥ä¸­å’Œ
$$Y=(Xdiag(s)^{-1}.(diag(s)W))=\hat{X}\hat{W}$$
ç›´è§‚çš„ç†è§£å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
<div align=center>
<img src="./assets/smoothquant2.png"/>
</div>

å·¦è¾¹ä¸ºsmoothå‰ï¼Œå³è¾¹ä¸ºsmoothåï¼Œå¯ä»¥æ˜æ˜¾çœ‹åˆ°Xä¹˜ä»¥$s^{-1}$ä¹‹åæ•°æ®åˆ†å¸ƒæ˜æ˜¾å‡åŒ€äº†ï¼ŒæŠŠéš¾åº¦åŒ€äº†ä¸€ç‚¹ç»™weightã€‚

`examples/llama`æš‚ä¸æ”¯æŒsmoothquantï¼Œè¿™é‡Œæˆ‘ä»¬å®ç°äº†`examples/llama`çš„smoothquant

ToDo:smoothquantå®ç°


3.inflight batching

<div align=center>
<img src="./assets/inflight-batch.png"/>
</div>

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä¸ºäº†å¢åŠ thoughputæˆ‘ä»¬å¸Œæœ›æ¯æ¬¡æ¨æ–­è¿›batchçš„æ•°æ®ï¼Œå¯¹äºLLMæ¥è¯´é¦–å…ˆéœ€è¦ä¸€ä¸ªbatchçš„æ•°æ®ä¸­é•¿åº¦ä¸åŒçš„sequenceè¿›è¡Œpaddingåˆ°ç›¸åŒçš„é•¿åº¦ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºbatch size=3,é»‘è‰²çš„çŸ©å½¢æ¡†è¡¨ç¤ºå¯¹æ¯ä¸ªsequenceè¿›è¡Œçš„padding,å¦‚æœä¸è¿›è¡Œinflight batchingæ“ä½œï¼Œä¸€ä¸ªbatchçš„æ•°æ®å¿…é¡»å…¨éƒ¨generateå®Œæˆæ‰èƒ½ä¸€èµ·è¿”å›ï¼Œè€Œè¯¥bacthä¸­å³ä½¿æœ‰æå‰generateå®Œæˆçš„sequenceä¹Ÿå¿…é¡»ç­‰å¾…ã€‚

inflight batchingçš„è¿‡ç¨‹å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œé¦–å…ˆæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªRequest Waiting Poolè¿™é‡Œå­˜æ”¾äº†æ‰€æœ‰çš„å¾…æ¨æ–­çš„sequenceï¼Œå‡è®¾æœ‰ä¸€ä¸ªbatchçš„æ•°æ®paddingåç»è¿‡context phaseè¿›è¡Œgeneration phaseï¼Œbatchä¸­çš„ç¬¬2ä¸ªæ•°æ®æå‰generateå®Œåå³åˆ»è¿”å›ç»“æœï¼Œæ­¤æ—¶å¯ä»¥åœ¨Request Waiting Poolä¸­å–å‡ºè“è‰²çš„æ–°sequenceåŠ å…¥åˆ°å½“å‰batchä¸­ï¼Œè“è‰²çš„sequenceæ‰§è¡Œcontext phaseè¿›è€Œæ‰§è¡Œgeneration phase,batchä¸­çš„å…¶ä»–æ•°æ®ç»§ç»­æ‰§è¡Œgeneration phaseã€‚é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œç›´åˆ°Poolä¸­æ— éœ€è¦æ¨æ–­çš„æ•°æ®ä¸ºæ­¢ã€‚

ä¸‹é¢æˆ‘ä»¬å°†å°è¯•åœ¨`examples/llama`ä¸­å®ç°inflight batching:

ToDo:inflight batching


### 3.ä¼˜åŒ–æ•ˆæœ
---

ToDo

### 4.BugæŠ¥å‘Š
---

<div align=center>

|:bug: Bugåç§°|Issue|æ˜¯å¦è¢«å®˜æ–¹ç¡®è®¤|è¯´æ˜|
|-|-|:-:|-|
|InstanceNormalization Plugin |<https://github.com/NVIDIA/TensorRT/issues/3165>||å®˜æ–¹æš‚æœªç¡®è®¤|

</div>

### 5.é€åˆ†é¢˜ç­”æ¡ˆ
---

> ğŸ” é—®é¢˜1ï¼šè¯·å†™å‡º `./tensorrt_llm_july-release-v1/examples/gpt/README` é‡Œé¢ `â€œSingle node, single GPUâ€` éƒ¨åˆ†å¦‚ä¸‹å‘½ä»¤çš„è¾“å‡ºï¼ˆ10åˆ†ï¼‰[æ¨¡å‹ä¸ºgpt2-medium](https://huggingface.co/gpt2-medium) 

```shell
python3 run.py --max_output_len=8 
```
<details>
<summary>ğŸ”‘ç‚¹æˆ‘æŸ¥çœ‹ é—®é¢˜1 è§£æ</summary>

0. å¿…è¦çš„Python Packageå®‰è£…

```shell
cd ./tensorrt_llm_july-release-v1/examples/gpt
pip3 install requirements.txt -i https://mirrors.aliyun.com/pypi/simple
```

1. ä¸‹è½½HuggingFace(HF)æ¨¡å‹

```shell
# ä¸‹è½½HFæ¨¡å‹
rm -rf gpt2 && git clone https://huggingface.co/gpt2-medium gpt2

# æ›´æ–°.binæ¨¡å‹
cd gpt2
rm pytorch_model.bin model.safetensors
wget https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin
cd ..
```
2. å°†HF weightè½¬ä¸ºFT weight

TensorRT-LLM å¯ä»¥ç›´æ¥åŠ è½½FastTransformer(FT)æ ¼å¼çš„æ¨¡å‹weightæ–‡ä»¶ï¼Œå› æ­¤éœ€è¦å°†HF weightè½¬æ¢ä¸ºFT weight

```shell
python3 hf_gpt_convert.py -i gpt2 -o ./c-model/gpt2 --tensor-parallelism 1 --storage-type float16
```
è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹è½¬æ¢å®Œæˆï¼Œå¹¶åœ¨`tensorrt_llm_july-release-v1/examples/gpt/c-model/gpt2/1-gpu`ä¸­å­˜æ”¾äº†ç”Ÿæˆåçš„FT weight

<div align=center>
<img src="./assets/section5/p1.png"/>
</div>

3. æ„å»ºTensorRT-LLM engine

TensorRT-LLM engineçš„æ„å»ºè¿‡ç¨‹ä½¿ç”¨äº†FT weightå’Œå¯¹åº”çš„é…ç½®æ–‡ä»¶ï¼ˆå·²ç»åœ¨ç¬¬2æ­¥ç”Ÿæˆï¼‰å’Œè‡ªå®šä¹‰çš„Tokenizerã€‚è¿‡ç¨‹ä¸­å¦‚æœä¸æŒ‡å®šæ¨¡å‹æƒé‡è·¯å¾„ï¼ŒTensorRT-LLMé»˜è®¤éšæœºåˆå§‹åŒ–è¿™äº›weightç”Ÿæˆengineã€‚

```shell
# single GPU float16 ä½¿ç”¨FT weightç”Ÿæˆengine
python3 build.py --model_dir=./c-model/gpt2/1-gpu --use_gpt_attention_plugin
```
æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹åºåˆ—åŒ–å®Œæˆï¼Œå¹¶å°†engineä¿å­˜åœ¨`./tensorrt_llm_july-release-v1/examples/gpt/gpt_outputs`ä¸­

```
[08/24/2023-07:07:38] [TRT-LLM] [I] Total time of building gpt_float16_tp1_rank0.engine: 00:00:35
[08/24/2023-07:07:38] [TRT-LLM] [I] Config saved to gpt_outputs/config.json.
[08/24/2023-07:07:38] [TRT-LLM] [I] Serializing engine to gpt_outputs/gpt_float16_tp1_rank0.engine...
[08/24/2023-07:07:42] [TRT-LLM] [I] Engine serialized. Total time: 00:00:04
[08/24/2023-07:07:42] [TRT-LLM] [I] Timing cache serialized to gpt_outputs/model.cache
[08/24/2023-07:07:42] [TRT-LLM] [I] Total time of building all 1 engines: 00:00:50
```
âš ï¸æ³¨æ„ï¼š `build.py`æ”¯æŒå¤šGPUå¹¶è¡Œæ„å»ºTensorRT-LLM engine,è¯¦ç»†çš„å¯ä»¥å‚è€ƒï¼š`./tensorrt_llm_july-release-v1/examples/gpt/README`

4. Single node,single GPUä¸‹æµ‹è¯•engine

```shell
# single GPUä¸‹è¿è¡Œengine
python3 run.py --max_output_len=8
```
è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
Input: Born in north-east France, Soyer trained as a
Output:  chef and eventually became a chef at a
```
é—®é¢˜1è§£æå®Œæˆã€‚

âš ï¸æ³¨æ„ï¼š`run.py`æ”¯æŒsingle node multiple GPUs(åŸºäº`mpirun`)å’Œmultiple nodes,multiple GPUs(åŸºäº[Slurm](https://slurm.schedmd.com))ï¼Œè¯¦ç»†çš„å¯ä»¥å‚è€ƒ`./tensorrt_llm_july-release-v1/examples/gpt/README`

</details>


> ğŸ” é—®é¢˜2: è¯·å†™å‡º `./tensorrt_llm_july-release-v1/examples/gpt/README` é‡Œé¢ `â€œSummarization using the GPT modelâ€` éƒ¨åˆ†å¦‚ä¸‹å‘½ä»¤çš„rouge åˆ†æ•°ï¼ˆ10åˆ†ï¼‰[æ¨¡å‹ä¸ºgpt2-medium](https://huggingface.co/gpt2-medium)

```shell
python3 summarize.py --engine_dirtrt_engine/gpt2/fp16/1-gpu --test_hf  --batch_size1  --test_trt_llm  --hf_model_location=gpt2 --check_accuracy --tensorrt_llm_rouge1_threshold=14
```

<details>
<summary>ğŸ”‘ç‚¹æˆ‘æŸ¥çœ‹ é—®é¢˜2 è§£æ</summary>

è¯¥é—®é¢˜å°†æè¿°å¦‚ä½•ä½¿ç”¨TensorRT-LLMè¿è¡Œä¸€ä¸ªæ–‡æœ¬æ‘˜è¦ä»»åŠ¡çš„GPT-2,è¿™é‡Œä½¿ç”¨çš„æ•°æ®é›†ä¸º[cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) ï¼Œå¯¹åº”ç”Ÿæˆçš„æ‘˜è¦ä½¿ç”¨[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) scoreæ¥è¯„ä»·TensorRT-LLMçš„ç²¾åº¦å˜åŒ–ï¼Œç¡®åˆ‡çš„è¯´è¿™é‡Œä½¿ç”¨äº†`ROUGE-1` scoreã€‚

å…³äºè¯„ä»·æŒ‡æ ‡[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric))çš„ä»‹ç»ï¼Œæˆ‘ä»¬æ¨èå‚è€ƒçŸ¥ä¹çš„ä»‹ç»ï¼š

+ [ä¸­æ–‡æ–‡æœ¬æ‘˜è¦æŒ‡æ ‡-ROUGE](https://zhuanlan.zhihu.com/p/388720967)
+ [NLPè¯„ä¼°æŒ‡æ ‡ä¹‹ROUGE](https://zhuanlan.zhihu.com/p/504279252)

é—®é¢˜1ä¸­å·²ç»å®Œæˆäº†å¿…è¦çš„packageçš„å®‰è£…ï¼Œè¿™é‡Œç›´æ¥ä¸‹è½½HFæ¨¡å‹

1. ä¸‹è½½HFæ¨¡å‹æ–‡ä»¶

```shell
# ä¸‹è½½æ¨¡å‹æ–‡ä»¶
rm -rf gpt2 && git clone https://huggingface.co/gpt2 gpt2

# æ›´æ–°.binæ¨¡å‹æ–‡ä»¶
cd gpt2
rm pytorch_model.bin model.safetensors
wget https://huggingface.co/gpt2/resolve/main/pytorch_model.bin
cd ..
```

2. å°†HF weightè½¬æ¢ä¸ºFT weight

```shell
python3 hf_gpt_convert.py -i gpt2 -o ./c-model/gpt2/fp16 --tensor-parallelism 1 --storage-type float16
```

è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹è½¬æ¢å®Œæˆï¼Œå¹¶åœ¨`tensorrt_llm_july-release-v1/examples/gpt/c-model/gpt2/fp16/1-gpu`ä¸­å­˜æ”¾äº†ç”Ÿæˆåçš„FT weight

<div align=center>
<img src="./assets/section5/p2.png"/>
</div>

3. æ„å»ºTensorRT-LLM engine

```shell
python3 build.py --model_dir=./c-model/gpt2/fp16/1-gpu \
                 --use_gpt_attention_plugin \
                 --use_gemm_plugin \
                 --use_layernorm_plugin \
                 --max_batch_size 8 \
                 --max_input_len 924 \
                 --max_output_len 100 \
                 --output_dir trt_engine/gpt2/fp16/1-gpu/ \
                 --hidden_act gelu
```

æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹åºåˆ—åŒ–å®Œæˆï¼Œå¹¶å°†engineä¿å­˜åœ¨`./tensorrt_llm_july-release-v1/examples/gpt/trt_engine/gpt2/fp16/1-gpu`ä¸­

```
[08/24/2023-07:34:42] [TRT-LLM] [I] Total time of building gpt_float16_tp1_rank0.engine: 00:01:32
[08/24/2023-07:34:42] [TRT-LLM] [I] Config saved to trt_engine/gpt2/fp16/1-gpu/config.json.
[08/24/2023-07:34:42] [TRT-LLM] [I] Serializing engine to trt_engine/gpt2/fp16/1-gpu/gpt_float16_tp1_rank0.engine...
[08/24/2023-07:34:43] [TRT-LLM] [I] Engine serialized. Total time: 00:00:01
[08/24/2023-07:34:43] [TRT-LLM] [I] Timing cache serialized to trt_engine/gpt2/fp16/1-gpu/model.cache
[08/24/2023-07:34:43] [TRT-LLM] [I] Total time of building all 1 engines: 00:01:43
```

4. TensorRT-LLMä¸‹æµ‹è¯•GPT-2æ–‡æœ¬æ‘˜è¦ä»»åŠ¡

```shell
python3 summarize.py --engine_dir trt_engine/gpt2/fp16/1-gpu \
                     --test_hf \
                     --batch_size 1 \
                     --test_trt_llm \
                     --hf_model_location=gpt2 \
                     --check_accuracy \
                     --tensorrt_llm_rouge1_threshold=14
```
æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œç»“æœå¦‚ä¸‹ï¼š

```
[08/24/2023-08:40:54] [TRT-LLM] [I] ---------------------------------------------------------
Downloading builder script: 5.60kB [00:00, 6.22MB/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1151 > 1024). Running this sequence through the model will result in indexing errors
[08/24/2023-08:41:14] [TRT-LLM] [I] TensorRT-LLM (total latency: 2.6481666564941406 sec)
[08/24/2023-08:41:14] [TRT-LLM] [I] TensorRT-LLM beam 0 result
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge1 : 15.361040799540035
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge2 : 3.854022269668396
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeL : 12.078455591738333
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeLsum : 13.547802733617264
[08/24/2023-08:41:14] [TRT-LLM] [I] Hugging Face (total latency: 10.39808702468872 sec)
[08/24/2023-08:41:14] [TRT-LLM] [I] HF beam 0 result
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge1 : 14.75593024343394
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge2 : 3.3647470801871733
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeL : 11.124766996533
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeLsum : 13.031128048110618
```

<div align=center>
<img src="./assets/section5/p3.png"/>
</div>

é—®é¢˜2è§£æå®Œæˆã€‚

âš ï¸æ³¨æ„ï¼šè¿‡ç¨‹ä¸­éœ€è¦ä¸‹è½½æ•°æ®ï¼Œå¦‚æœè¿è¡Œè¿‡ç¨‹ä¸­ä¸­æ–­æˆ–æŠ¥é”™å¤§å¤šæ˜¯å› ä¸ºç½‘ç»œåŸå› ï¼Œè¯·è€å¿ƒå¤šå°è¯•é‡å¤è¿è¡Œå‡ æ¬¡

</details>


### 6.æœªæ¥å·¥ä½œ
---

ToDo

### 7.ğŸ˜‰References
---

1. [TensorRT(Github)](https://github.com/NVIDIA/TensorRT)

2. [trt-samples-for-hackathon-cn](https://github.com/NVIDIA/trt-samples-for-hackathon-cn)

3. [NVIDIA TensorRT Hackathon 2023 â€”â€” ç”Ÿæˆå¼AIæ¨¡å‹ä¼˜åŒ–èµ›(å¤©æ± )](https://tianchi.aliyun.com/competition/entrance/532108/introduction?spm=a2c22.12281957.0.0.605a3b74bkLhBT)

4. [TensorRT 8.6 è®²åº§(Bç«™)](https://www.bilibili.com/video/BV1jj411Z7wG/)

5. [Llama(Github)](https://github.com/facebookresearch/llama)

6. [Llama paper (arxiv)](https://arxiv.org/abs/2302.13971)

7. [TensorRT-LLM:å¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼šä½ç²¾åº¦æœ€ä½³å®è·µ(Bç«™)](https://www.bilibili.com/video/BV1h44y1c72B/?share_source=copy_web&vd_source=db3eecb1b88cc6c7a18eeaf6db1ed114)

8. [TensorRT-LLMå¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼šä¼˜åŒ–å…³é”®æŠ€æœ¯è§£æ(Bç«™)](https://www.bilibili.com/video/BV1j44y1c7fT/?spm_id_from=333.788&vd_source=def8c63d9c5f9bf987870bf827bfcb3d)

9. [SmoothQuant(arxiv)](https://arxiv.org/abs/2211.10438)


