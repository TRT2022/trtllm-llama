<!-- <img src="docs/logo.png" align="right" alt="logo" height="180"  /> -->
<img src="assets/trt2023.jpeg" align="center" alt="logo"  />

## åŸºäºTensorRT-LLMçš„LLaMAæ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ :zap:
### LLaMA: Open and Efficient Foundation Language Models for TensorRT Hackathon 2023 <img src="assets/llama.png" alt="logo"  width=4%/>

[![](https://img.shields.io/badge/Github-TensorRT-blue)](https://github.com/NVIDIA/TensorRT)
[![](https://img.shields.io/badge/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0-TensorRT%20Hackathon%202023-blue)](https://tianchi.aliyun.com/competition/entrance/532108/introduction?spm=a2c22.12281957.0.0.4c885d9bOexwJc)
[![](https://img.shields.io/badge/NVIDIA-TensorRT%20CookBook%20CN-blue)](https://github.com/NVIDIA/trt-samples-for-hackathon-cn)
[![](https://img.shields.io/badge/B%E7%AB%99-GodV%20TensorRT%E6%95%99%E7%A8%8B-blue)](https://www.bilibili.com/video/BV1jj411Z7wG/?spm=a2c22.12281978.0.0.49ed2274CQCrY7)
[![](https://img.shields.io/badge/Github-%20%E5%88%9D%E8%B5%9B%E6%80%BB%E7%BB%93-blue)](https://github.com/TRT2022/ControlNet_TensorRT)
[![](https://img.shields.io/badge/Github-LLaMA-blue)](https://github.com/facebookresearch/llama)


:alien: : **ç¾è¿ªåº·-åŒ—èˆªAI Lab** 

### 0.â³æ—¥å¿—

<div align=center>

|æ—¶é—´ç‚¹|æäº¤å†…å®¹|è¯´æ˜|
|-|-|-|
|2023-08-21|å’ŒNVIDIAå¯¼å¸ˆå›¢é˜Ÿç¡®å®šä¼˜åŒ–æ–¹æ¡ˆï¼šåŸºäºå¼€æºLLMçš„LLaMAæ¨¡å‹æ¨æ–­åŠ é€Ÿä¼˜åŒ–|é€‰é¢˜|
|2023-08-22|åˆ›å»ºGithubé¡¹ç›®                                              |é¡¹ç›®åˆ›å»º|
|2023-08-24|å®Œæˆé€åˆ†é¢˜ä½œç­”                                               |é€åˆ†é¢˜ |
|2023-08-31|examples/llamaæºç å­¦ä¹                                        |é¡¹ç›®åˆ†æ |
|2023-09-3|æ­£å¸¸è¿è¡Œexamples/llama                                     |ä»£ç è¿è¡Œæµ‹è¯• |


â˜£ï¸å¤èµ›è°ƒä¼˜é˜¶æ®µï¼š2023å¹´8æœˆ17æ—¥-9æœˆ21æ—¥
</div>


### 1.æ€»è¿°
---

ä½œä¸º [NVIDIA TensorRT Hackathon 2023](https://github.com/NVIDIA/trt-samples-for-hackathon-cn/tree/master/Hackathon2023) çš„å¤èµ›å‚èµ›é¢˜ç›®ï¼Œæœ¬å·¥ä½œåŸºäºTensorRT-LLMä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹LLaMAã€‚æŒ‰ç…§å‚èµ›è¦æ±‚æœ¬é¡¹ç›®é€‰æ‹©å®Œæˆ**TensorRT-LLMè¯•ç”¨é€åˆ†é¢˜**ä»¥åŠ**3+4æ¨¡å¼çš„TensorRT-LLMæ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ**ï¼Œå³ï¼š
+ 3ï¼šç”¨TensorRT-LLMä¼˜åŒ–examplesç›®å½•ä¸‹çš„æŸä¸ªç°æœ‰æ¨¡å‹ï¼ˆæœ¬é¡¹ç›®ä¼˜åŒ–`examples/llama`æ¨¡å‹ï¼‰
+ 4ï¼šå°è¯•ä¸ºTensorRT-LLMæ·»åŠ æ–°featureï¼Œæˆ–è€…åœ¨æ¨¡å‹ä¸Šå¯ç”¨äº†ç°æœ‰feature

ä¸‹é¢æˆ‘ä»¬å°†ä»æ¨¡å‹ä»‹ç»ï¼Œä¼˜åŒ–æ•ˆæœåŠå¦‚ä½•è¿è¡Œè¯¥é¡¹ç›®ç­‰3ä¸ªæ–¹é¢ä»‹ç»æœ¬é¡¹ç›®

é¦–å…ˆæ˜¯LLaMAæ¨¡å‹çš„ç›¸å…³ä»‹ç»ï¼Œ[LLaMA](https://github.com/facebookresearch/llama) æ˜¯ç›®å‰ä¸ºæ­¢ï¼Œæ•ˆæœæœ€å¥½çš„å¼€æº LLM ä¹‹ä¸€,æ•°æ®é›†å±‚é¢ä¸Šå…±æœ‰1.4Tçš„Tokens, tokenizerä½¿ç”¨byte pair encoding (BPE) ç®—æ³•ï¼ŒSentence-Pieceçš„å®ç°,æ‰€æœ‰æ•°å­—è¢«æ‹†åˆ†ä¸ºå•ç‹¬çš„digitï¼Œæ‰€æœ‰æœªçŸ¥çš„UTF-8 å­—ç¬¦ï¼Œå›é€€åˆ°å­—èŠ‚æ¥è¿›è¡Œåˆ†è§£ã€‚å› æ­¤ï¼ŒLLaMA å¯ä»¥é€šè¿‡byte çš„æ–¹å¼ï¼Œæ„é€ å‡ºå¾ˆå¤šä¸åœ¨ vocab ä¸­çš„å­—ç¬¦ï¼Œä»è€Œä¹Ÿå…·æœ‰è¾ƒå¥½çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚ç½‘ç»œç»“æ„ä¸Šçš„æ”¹è¿›åŸºäºTransformerçš„æ¶æ„ï¼Œå¹¶åšäº†å¦‚ä¸‹3ç‚¹æ”¹è¿›ï¼š
+ Pre-Normalizationï¼šä¸ºäº†æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¯¹æ¯ä¸ªtransformerå±‚çš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ï¼ˆä½¿ç”¨ RMS Norm å½’ä¸€åŒ–å‡½æ•°ï¼‰
+ SwiGLUï¼šä½¿ç”¨SwiGLUæ›¿ä»£äº†ReLUä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚å’ŒPaLMä¸­ä¸åŒï¼Œç»´åº¦é‡‡ç”¨$\frac{2}{3}4d$è€Œä¸æ˜¯$4d$  
+ RoPEï¼šé‡‡ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼Œä½¿å¾—å¤§æ¨¡å‹çš„ç”Ÿæˆæœ‰æ›´å¥½çš„å¤–æ¨æ€§

LLaMA-13B ä¼˜äº GPT-3ï¼Œå°½ç®¡åªæœ‰1/10å¤§å°ã€‚ LLaMA-65B æ˜¯å¯ä»¥ä¸ Chinchilla-70B å’Œ PaLM-540B è¿™ç§æœ€ä½³çš„LLMç›¸ç«äº‰çš„æ¨¡å‹ã€‚ç»è¿‡å¾®è°ƒä¹‹åï¼ŒLLaMAçš„æ•ˆæœæœ‰æ˜¾è‘—çš„æå‡ã€‚å…³äºLLaMAçš„ä»‹ç»ï¼Œæ¨èçŸ¥ä¹æ–‡ç« ï¼š
+ [LLaMA è¶…è¯¦ç»†è§£è¯»ï¼ˆpaper & codeï¼‰](https://zhuanlan.zhihu.com/p/632102048?utm_id=0)

é’ˆå¯¹äºLLaMA-7Bå’ŒTensorRT-LLMä¸‹çš„`examples/llama`,æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹æ¡ˆè®¡åˆ’å®ç°è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<div align=center>
<img src="./assets/TensorRT-LLM LLaMaè¿›åº¦å›¾.png"/>
</div>

ToDoï¼š
- ä¼˜åŒ–æ•ˆæœï¼ˆä¾‹å¦‚ç»™å‡ºç²¾åº¦å’ŒåŠ é€Ÿæ¯”ï¼‰ï¼Œç®€å•ç»™å‡ºå…³é”®çš„æ•°å­—å³å¯ï¼Œåœ¨è¿™é‡Œä¸å¿…è¯¦ç»†å±•å¼€
- åœ¨Dockeré‡Œé¢ä»£ç ç¼–è¯‘ã€è¿è¡Œæ­¥éª¤çš„å®Œæ•´è¯´æ˜
  - è¯·åšåˆ°åªè¦é€è¡Œè¿è¡Œä½ ç»™çš„å‘½ä»¤ï¼Œå°±èƒ½æŠŠä»£ç è·‘èµ·æ¥


### 2.ä¸»è¦å¼€å‘å·¥ä½œ
---

#### 2.1 å¼€å‘å·¥ä½œçš„éš¾ç‚¹

è¯¥æ¨¡å‹çš„ä¼˜åŒ–éš¾ç‚¹å¦‚ä¸‹ï¼š

+ å¯¹äºTensorRT-LLMç›¸å…³åŠŸèƒ½å’ŒAPIä¸ç†Ÿæ‚‰
+ `examples/llama`ä¸­å·²ç»å®ç°äº†å¤šæ•°çš„featureï¼Œåœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥çš„ä¼˜åŒ–éš¾åº¦è¾ƒå¤§

ä½†æ˜¯ï¼ŒLLaMAä½œä¸ºç›¸å¯¹è¾ƒæ—©çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œå…¶ç›´æ¥å½±å“äº†å›½å†…å¤–å¤§æ¨¡å‹çš„å‘å±•å’Œç ”å‘æ€è·¯ï¼Œå…¶é‡è¦æ€§ä¸è¨€è€Œå–»ï¼Œé’ˆå¯¹äºLLaMAçš„TensorRT-LLMæ¨¡å‹ä¼˜åŒ–æ„ä¹‰é‡å¤§ã€‚

#### 2.2 å¼€å‘ä¸ä¼˜åŒ–è¿‡ç¨‹

é’ˆå¯¹äºLLaMA-7Bæˆ‘ä»¬çš„ä¼˜åŒ–è¿‡ç¨‹ä¸»è¦åˆ†ä»¥ä¸‹3ä¸ªéƒ¨åˆ†ï¼š
+ 1.åˆæ­¥è¿è¡Œ`examples/llama`é¡¹ç›®
+ 2.nsight systemåˆ†æé€æ­¥æ·»åŠ featureè¿›è¡Œæ¶ˆèå®éªŒ
+ 3.æ–°featuteå®ç°ï¼šinflight batching å’Œ smoothquant

æ¯ä¸€éƒ¨åˆ†æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„è¿è¡Œè„šæœ¬å’Œæµ‹è¯•ç»“æœã€‚

##### 2.2.1 åˆæ­¥è¿è¡Œ`examples/llama`é¡¹ç›®

0. å‡†å¤‡LLaMA-7B meta checkpointæ¨¡å‹

+ ä¸‹è½½æ¨¡å‹

LlaMA-7B v1 (meta checkpoint)æ¨¡å‹ä¸‹è½½åœ°å€ï¼š <https://115.com/s/sw6a2kv3w4z?password=a835&#>,å°†ä¸‹è½½åçš„æ¨¡å‹å­˜æ”¾åœ¨`/tensorrt_llm_july-release-v1/examples/llama/llama-1-7b-meta/`ä¸‹

+ meta checkpoint è½¬ huggingface(HF) checkpoint

```shell
# cdåˆ°ç›®æ ‡è·¯å¾„
cd cd ./tensorrt_llm_july-release-v1/examples/llama
# æ¨¡å‹è½¬HF checkpoint
python3 /usr/local/lib/python3.8/dist-packages/transformers/models/llama/convert_llama_weights_to_hf.py  --input_dir ./llama-1-7b-meta --model_size 7B --output_dir ./tmp/llama/7B
```

1. æ„å»ºTensorRT-LLM engine

```shell
# ä¸åŠ å…¥ä»»ä½•trick
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu \
                --visualize

# åŠ å…¥plugin
python build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu/
```

2. è¿è¡Œengine

```shell
python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/
```

3. ä½¿ç”¨LLaMA-7Bæµ‹è¯•æ–‡æœ¬æ‘˜è¦ä»»åŠ¡

```shell
# ä½¿ç”¨TensorRT-LLM engineæµ‹è¯•
python3 summarize.py --test_trt_llm \
                    --hf_model_location ./tmp/llama/7B/ \
                    --data_type fp16 \
                    --engine_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu/
```
ç»“æœï¼š

```
[09/03/2023-13:56:21] [TRT-LLM] [I] ---------------------------------------------------------
[09/03/2023-13:57:32] [TRT-LLM] [I] TensorRT-LLM (total latency: 65.88509821891785 sec)
[09/03/2023-13:57:32] [TRT-LLM] [I] TensorRT-LLM beam 0 result
[09/03/2023-13:57:32] [TRT-LLM] [I]   rouge1 : 19.478572394974464
[09/03/2023-13:57:32] [TRT-LLM] [I]   rouge2 : 5.748473587185184
[09/03/2023-13:57:32] [TRT-LLM] [I]   rougeL : 14.488586709461371
[09/03/2023-13:57:32] [TRT-LLM] [I]   rougeLsum : 17.818188740969955
```

```shell
# ä½¿ç”¨HFæ¨¡å‹æµ‹è¯•
python3 summarize.py --test_hf \
                    --hf_model_location ./tmp/llama/7B/ \
                    --data_type fp16 
```

ç»“æœï¼š

```
[09/03/2023-14:02:07] [TRT-LLM] [I] ---------------------------------------------------------
[09/03/2023-14:03:30] [TRT-LLM] [I] Hugging Face (total latency: 78.22841620445251 sec)
[09/03/2023-14:03:30] [TRT-LLM] [I] HF beam 0 result
[09/03/2023-14:03:30] [TRT-LLM] [I]   rouge1 : 20.106338916310662
[09/03/2023-14:03:30] [TRT-LLM] [I]   rouge2 : 5.910110463256421
[09/03/2023-14:03:30] [TRT-LLM] [I]   rougeL : 15.2269090887293
[09/03/2023-14:03:30] [TRT-LLM] [I]   rougeLsum : 17.938095329383458
```

æˆ‘ä»¬åˆæ­¥æ­£å¸¸è¿è¡Œäº†`example/llama`,ä»ç»“æœä¸Šåˆæ­¥éªŒè¯äº†æ­£ç¡®æ€§ã€‚

##### 2.2.2 nsight systemåˆ†æé€æ­¥æ·»åŠ featureè¿›è¡Œæ¶ˆèå®éªŒ

è¯¥éƒ¨åˆ†æˆ‘ä»¬åšäº†è¯¦ç»†çš„æ¶ˆèå®éªŒï¼Œé€šè¿‡é€æ­¥æ·»åŠ featureå’Œtrickçš„æ–¹å¼éªŒè¯ä¸åŒfeatureåœ¨LLaMA-7Bä¸Šçš„å»¶æ—¶çš„æ”¶ç›Šï¼Œå¹¶åŸºäºnsight systemè¿›è¡Œprofilingã€‚

ç›®å‰`examples/llama`çš„featureæ”¯æŒæƒ…å†µå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<div align=center>
<img src="./assets/TensorRT_LLM LLaMa.png"/>
</div>

ç”±äºLLaMA-7Bä¸­ä½¿ç”¨äº†RoPE,ç›®å‰`gpt_attention_plugin`æ˜¯å”¯ä¸€çš„ä¸€ç§æ”¯æŒRoPEçš„æ–¹å¼ï¼Œå› æ­¤LLaMAåœ¨TensorRT-LLMä¸­å¼ºåˆ¶ä½¿ç”¨äº†`gpt_attention_plugin`

1. æ·»åŠ : k/v cache + attention pligin

+ build engine

```shell
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu \
```

+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
# è¿è¡Œengine
nsys profile -o trt_llm_only_kv_cache_fp16 python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/

# è¿è¡ŒHF checkpoint
python3 run_hf.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --hf_model_location ./tmp/llama/7B/
```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 1.404158673286438 sec)
# HF
llama-hf-run (mean latency: 1.7185083055496215 sec)
```
ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `k/v cache + attention plugin`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.40416ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`1.224`

åˆ†æå¯¼å‡ºçš„`trt_llm_only_kv_cache_fp16` nsysæ–‡ä»¶ï¼Œå¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°attention pluginå’Œk/v cacheçš„æ¨ç†å»¶æ—¶æƒ…å†µï¼š

ToDo

2. æ·»åŠ : k/v cache + attention_plugin + weight_only_quant

+ build engine

```shell
python build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_weight_only \
                --output_dir ./tmp/llama/7B/trt_engines/weight_only/1-gpu/
```

+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
nsys profile -o trt_llm_weight_only python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/weight_only/1-gpu/
```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 0.7849386262893677 sec)
```
ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `k/v cache + attention plugin + weight_only_quant`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`0.78494ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`2.189`

ToDo nsys

3. æ·»åŠ : int8 k/v cache + attention_plugin + weight_only_quant 

+ build engine

```shell
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_int8_kv_cache \
                --output_dir ./tmp/llama/7B/trt_engines/int8_kvcache/1-gpu/

```

+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
nsys profile -o trt_llm_weight_only_int8_kvcache python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/int8_kvcache/1-gpu/
```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 0.7858470821380615 sec)
```
ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `int8 k/v cache + attention plugin + weight_only_quant`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`0.78585ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`2.187`

ToDo nsys

4. æ·»åŠ : k/v-cache + attention plugin + weight_only_quant + gemm plugin

+ build engine

```shell

python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --use_weight_only \
                --output_dir ./tmp/llama/7B/trt_engines/weight_only_attention_gemm/1-gpu/

```

```shell
nsys profile -o trt_llm_weight_only_attention_gemm python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/weight_only_attention_gemm/1-gpu/

```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 0.7930449199676514 sec)
```
ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `int8 k/v cache + attention plugin + weight_only_quant + gemm plugin`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`0.79304ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`2.167`



#### 2.2.3 æ–°featuteå®ç°ï¼šinflight batching å’Œ smoothquant

ToDo


### 3.ä¼˜åŒ–æ•ˆæœ
---

ToDo

### 4.BugæŠ¥å‘Š
---

<div align=center>

|:bug: Bugåç§°|Issue|æ˜¯å¦è¢«å®˜æ–¹ç¡®è®¤|è¯´æ˜|
|-|-|:-:|-|
|InstanceNormalization Plugin |<https://github.com/NVIDIA/TensorRT/issues/3165>||å®˜æ–¹æš‚æœªç¡®è®¤|

</div>

### 5.é€åˆ†é¢˜ç­”æ¡ˆ
---

> ğŸ” é—®é¢˜1ï¼šè¯·å†™å‡º `./tensorrt_llm_july-release-v1/examples/gpt/README` é‡Œé¢ `â€œSingle node, single GPUâ€` éƒ¨åˆ†å¦‚ä¸‹å‘½ä»¤çš„è¾“å‡ºï¼ˆ10åˆ†ï¼‰[æ¨¡å‹ä¸ºgpt2-medium](https://huggingface.co/gpt2-medium) 

```shell
python3 run.py --max_output_len=8 
```
<details>
<summary>ğŸ”‘ç‚¹æˆ‘æŸ¥çœ‹ é—®é¢˜1 è§£æ</summary>

0. å¿…è¦çš„Python Packageå®‰è£…

```shell
cd ./tensorrt_llm_july-release-v1/examples/gpt
pip3 install requirements.txt -i https://mirrors.aliyun.com/pypi/simple
```

1. ä¸‹è½½HuggingFace(HF)æ¨¡å‹

```shell
# ä¸‹è½½HFæ¨¡å‹
rm -rf gpt2 && git clone https://huggingface.co/gpt2-medium gpt2

# æ›´æ–°.binæ¨¡å‹
cd gpt2
rm pytorch_model.bin model.safetensors
wget https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin
cd ..
```
2. å°†HF weightè½¬ä¸ºFT weight

TensorRT-LLM å¯ä»¥ç›´æ¥åŠ è½½FastTransformer(FT)æ ¼å¼çš„æ¨¡å‹weightæ–‡ä»¶ï¼Œå› æ­¤éœ€è¦å°†HF weightè½¬æ¢ä¸ºFT weight

```shell
python3 hf_gpt_convert.py -i gpt2 -o ./c-model/gpt2 --tensor-parallelism 1 --storage-type float16
```
è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹è½¬æ¢å®Œæˆï¼Œå¹¶åœ¨`tensorrt_llm_july-release-v1/examples/gpt/c-model/gpt2/1-gpu`ä¸­å­˜æ”¾äº†ç”Ÿæˆåçš„FT weight

<div align=center>
<img src="./assets/section5/p1.png"/>
</div>

3. æ„å»ºTensorRT-LLM engine

TensorRT-LLM engineçš„æ„å»ºè¿‡ç¨‹ä½¿ç”¨äº†FT weightå’Œå¯¹åº”çš„é…ç½®æ–‡ä»¶ï¼ˆå·²ç»åœ¨ç¬¬2æ­¥ç”Ÿæˆï¼‰å’Œè‡ªå®šä¹‰çš„Tokenizerã€‚è¿‡ç¨‹ä¸­å¦‚æœä¸æŒ‡å®šæ¨¡å‹æƒé‡è·¯å¾„ï¼ŒTensorRT-LLMé»˜è®¤éšæœºåˆå§‹åŒ–è¿™äº›weightç”Ÿæˆengineã€‚

```shell
# single GPU float16 ä½¿ç”¨FT weightç”Ÿæˆengine
python3 build.py --model_dir=./c-model/gpt2/1-gpu --use_gpt_attention_plugin
```
æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹åºåˆ—åŒ–å®Œæˆï¼Œå¹¶å°†engineä¿å­˜åœ¨`./tensorrt_llm_july-release-v1/examples/gpt/gpt_outputs`ä¸­

```
[08/24/2023-07:07:38] [TRT-LLM] [I] Total time of building gpt_float16_tp1_rank0.engine: 00:00:35
[08/24/2023-07:07:38] [TRT-LLM] [I] Config saved to gpt_outputs/config.json.
[08/24/2023-07:07:38] [TRT-LLM] [I] Serializing engine to gpt_outputs/gpt_float16_tp1_rank0.engine...
[08/24/2023-07:07:42] [TRT-LLM] [I] Engine serialized. Total time: 00:00:04
[08/24/2023-07:07:42] [TRT-LLM] [I] Timing cache serialized to gpt_outputs/model.cache
[08/24/2023-07:07:42] [TRT-LLM] [I] Total time of building all 1 engines: 00:00:50
```
âš ï¸æ³¨æ„ï¼š `build.py`æ”¯æŒå¤šGPUå¹¶è¡Œæ„å»ºTensorRT-LLM engine,è¯¦ç»†çš„å¯ä»¥å‚è€ƒï¼š`./tensorrt_llm_july-release-v1/examples/gpt/README`

4. Single node,single GPUä¸‹æµ‹è¯•engine

```shell
# single GPUä¸‹è¿è¡Œengine
python3 run.py --max_output_len=8
```
è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
Input: Born in north-east France, Soyer trained as a
Output:  chef and eventually became a chef at a
```
é—®é¢˜1è§£æå®Œæˆã€‚

âš ï¸æ³¨æ„ï¼š`run.py`æ”¯æŒsingle node multiple GPUs(åŸºäº`mpirun`)å’Œmultiple nodes,multiple GPUs(åŸºäº[Slurm](https://slurm.schedmd.com))ï¼Œè¯¦ç»†çš„å¯ä»¥å‚è€ƒ`./tensorrt_llm_july-release-v1/examples/gpt/README`

</details>


> ğŸ” é—®é¢˜2: è¯·å†™å‡º `./tensorrt_llm_july-release-v1/examples/gpt/README` é‡Œé¢ `â€œSummarization using the GPT modelâ€` éƒ¨åˆ†å¦‚ä¸‹å‘½ä»¤çš„rouge åˆ†æ•°ï¼ˆ10åˆ†ï¼‰[æ¨¡å‹ä¸ºgpt2-medium](https://huggingface.co/gpt2-medium)

```shell
python3 summarize.py --engine_dirtrt_engine/gpt2/fp16/1-gpu --test_hf  --batch_size1  --test_trt_llm  --hf_model_location=gpt2 --check_accuracy --tensorrt_llm_rouge1_threshold=14
```

<details>
<summary>ğŸ”‘ç‚¹æˆ‘æŸ¥çœ‹ é—®é¢˜2 è§£æ</summary>

è¯¥é—®é¢˜å°†æè¿°å¦‚ä½•ä½¿ç”¨TensorRT-LLMè¿è¡Œä¸€ä¸ªæ–‡æœ¬æ‘˜è¦ä»»åŠ¡çš„GPT-2,è¿™é‡Œä½¿ç”¨çš„æ•°æ®é›†ä¸º[cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) ï¼Œå¯¹åº”ç”Ÿæˆçš„æ‘˜è¦ä½¿ç”¨[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) scoreæ¥è¯„ä»·TensorRT-LLMçš„ç²¾åº¦å˜åŒ–ï¼Œç¡®åˆ‡çš„è¯´è¿™é‡Œä½¿ç”¨äº†`ROUGE-1` scoreã€‚

å…³äºè¯„ä»·æŒ‡æ ‡[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric))çš„ä»‹ç»ï¼Œæˆ‘ä»¬æ¨èå‚è€ƒçŸ¥ä¹çš„ä»‹ç»ï¼š

+ [ä¸­æ–‡æ–‡æœ¬æ‘˜è¦æŒ‡æ ‡-ROUGE](https://zhuanlan.zhihu.com/p/388720967)
+ [NLPè¯„ä¼°æŒ‡æ ‡ä¹‹ROUGE](https://zhuanlan.zhihu.com/p/504279252)

é—®é¢˜1ä¸­å·²ç»å®Œæˆäº†å¿…è¦çš„packageçš„å®‰è£…ï¼Œè¿™é‡Œç›´æ¥ä¸‹è½½HFæ¨¡å‹

1. ä¸‹è½½HFæ¨¡å‹æ–‡ä»¶

```shell
# ä¸‹è½½æ¨¡å‹æ–‡ä»¶
rm -rf gpt2 && git clone https://huggingface.co/gpt2 gpt2

# æ›´æ–°.binæ¨¡å‹æ–‡ä»¶
cd gpt2
rm pytorch_model.bin model.safetensors
wget https://huggingface.co/gpt2/resolve/main/pytorch_model.bin
cd ..
```

2. å°†HF weightè½¬æ¢ä¸ºFT weight

```shell
python3 hf_gpt_convert.py -i gpt2 -o ./c-model/gpt2/fp16 --tensor-parallelism 1 --storage-type float16
```

è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹è½¬æ¢å®Œæˆï¼Œå¹¶åœ¨`tensorrt_llm_july-release-v1/examples/gpt/c-model/gpt2/fp16/1-gpu`ä¸­å­˜æ”¾äº†ç”Ÿæˆåçš„FT weight

<div align=center>
<img src="./assets/section5/p2.png"/>
</div>

3. æ„å»ºTensorRT-LLM engine

```shell
python3 build.py --model_dir=./c-model/gpt2/fp16/1-gpu \
                 --use_gpt_attention_plugin \
                 --use_gemm_plugin \
                 --use_layernorm_plugin \
                 --max_batch_size 8 \
                 --max_input_len 924 \
                 --max_output_len 100 \
                 --output_dir trt_engine/gpt2/fp16/1-gpu/ \
                 --hidden_act gelu
```

æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹åºåˆ—åŒ–å®Œæˆï¼Œå¹¶å°†engineä¿å­˜åœ¨`./tensorrt_llm_july-release-v1/examples/gpt/trt_engine/gpt2/fp16/1-gpu`ä¸­

```
[08/24/2023-07:34:42] [TRT-LLM] [I] Total time of building gpt_float16_tp1_rank0.engine: 00:01:32
[08/24/2023-07:34:42] [TRT-LLM] [I] Config saved to trt_engine/gpt2/fp16/1-gpu/config.json.
[08/24/2023-07:34:42] [TRT-LLM] [I] Serializing engine to trt_engine/gpt2/fp16/1-gpu/gpt_float16_tp1_rank0.engine...
[08/24/2023-07:34:43] [TRT-LLM] [I] Engine serialized. Total time: 00:00:01
[08/24/2023-07:34:43] [TRT-LLM] [I] Timing cache serialized to trt_engine/gpt2/fp16/1-gpu/model.cache
[08/24/2023-07:34:43] [TRT-LLM] [I] Total time of building all 1 engines: 00:01:43
```

4. TensorRT-LLMä¸‹æµ‹è¯•GPT-2æ–‡æœ¬æ‘˜è¦ä»»åŠ¡

```shell
python3 summarize.py --engine_dir trt_engine/gpt2/fp16/1-gpu \
                     --test_hf \
                     --batch_size 1 \
                     --test_trt_llm \
                     --hf_model_location=gpt2 \
                     --check_accuracy \
                     --tensorrt_llm_rouge1_threshold=14
```
æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œç»“æœå¦‚ä¸‹ï¼š

```
[08/24/2023-08:40:54] [TRT-LLM] [I] ---------------------------------------------------------
Downloading builder script: 5.60kB [00:00, 6.22MB/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1151 > 1024). Running this sequence through the model will result in indexing errors
[08/24/2023-08:41:14] [TRT-LLM] [I] TensorRT-LLM (total latency: 2.6481666564941406 sec)
[08/24/2023-08:41:14] [TRT-LLM] [I] TensorRT-LLM beam 0 result
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge1 : 15.361040799540035
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge2 : 3.854022269668396
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeL : 12.078455591738333
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeLsum : 13.547802733617264
[08/24/2023-08:41:14] [TRT-LLM] [I] Hugging Face (total latency: 10.39808702468872 sec)
[08/24/2023-08:41:14] [TRT-LLM] [I] HF beam 0 result
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge1 : 14.75593024343394
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge2 : 3.3647470801871733
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeL : 11.124766996533
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeLsum : 13.031128048110618
```

<div align=center>
<img src="./assets/section5/p3.png"/>
</div>

é—®é¢˜2è§£æå®Œæˆã€‚

âš ï¸æ³¨æ„ï¼šè¿‡ç¨‹ä¸­éœ€è¦ä¸‹è½½æ•°æ®ï¼Œå¦‚æœè¿è¡Œè¿‡ç¨‹ä¸­ä¸­æ–­æˆ–æŠ¥é”™å¤§å¤šæ˜¯å› ä¸ºç½‘ç»œåŸå› ï¼Œè¯·è€å¿ƒå¤šå°è¯•é‡å¤è¿è¡Œå‡ æ¬¡

</details>


### 6.æœªæ¥å·¥ä½œ
---

ToDo

### 7.ğŸ˜‰References
---

1. [TensorRT(Github)](https://github.com/NVIDIA/TensorRT)

2. [trt-samples-for-hackathon-cn](https://github.com/NVIDIA/trt-samples-for-hackathon-cn)

3. [NVIDIA TensorRT Hackathon 2023 â€”â€” ç”Ÿæˆå¼AIæ¨¡å‹ä¼˜åŒ–èµ›(å¤©æ± )](https://tianchi.aliyun.com/competition/entrance/532108/introduction?spm=a2c22.12281957.0.0.605a3b74bkLhBT)

4. [TensorRT 8.6 è®²åº§(Bç«™)](https://www.bilibili.com/video/BV1jj411Z7wG/)

5. [Llama(Github)](https://github.com/facebookresearch/llama)

6. [Llama paper (arxiv)](https://arxiv.org/abs/2302.13971)

7. [TensorRT-LLM:å¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼šä½ç²¾åº¦æœ€ä½³å®è·µ(Bç«™)](https://www.bilibili.com/video/BV1h44y1c72B/?share_source=copy_web&vd_source=db3eecb1b88cc6c7a18eeaf6db1ed114)

8. [å¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼šä½ç²¾åº¦æœ€ä½³å®è·µ(Bç«™)](https://www.bilibili.com/video/BV1h44y1c72B/?share_source=copy_web&vd_source=db3eecb1b88cc6c7a18eeaf6db1ed114)


