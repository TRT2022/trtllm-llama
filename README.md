<!-- <img src="docs/logo.png" align="right" alt="logo" height="180"  /> -->
<img src="assets/trt2023.jpeg" align="center" alt="logo"  />

## åŸºäºTensorRT-LLMçš„LLaMAæ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ :zap:
### LLaMA: Open and Efficient Foundation Language Models for TensorRT Hackathon 2023 <img src="assets/llama.png" alt="logo"  width=4%/>

[![](https://img.shields.io/badge/Github-TensorRT-blue)](https://github.com/NVIDIA/TensorRT)
[![](https://img.shields.io/badge/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0-TensorRT%20Hackathon%202023-blue)](https://tianchi.aliyun.com/competition/entrance/532108/introduction?spm=a2c22.12281957.0.0.4c885d9bOexwJc)
[![](https://img.shields.io/badge/NVIDIA-TensorRT%20CookBook%20CN-blue)](https://github.com/NVIDIA/trt-samples-for-hackathon-cn)
[![](https://img.shields.io/badge/B%E7%AB%99-GodV%20TensorRT%E6%95%99%E7%A8%8B-blue)](https://www.bilibili.com/video/BV1jj411Z7wG/?spm=a2c22.12281978.0.0.49ed2274CQCrY7)
[![](https://img.shields.io/badge/Github-%20%E5%88%9D%E8%B5%9B%E6%80%BB%E7%BB%93-blue)](https://github.com/TRT2022/ControlNet_TensorRT)
[![](https://img.shields.io/badge/Github-LLaMA-blue)](https://github.com/facebookresearch/llama)


:alien: : **ç¾è¿ªåº·-åŒ—èˆªAI Lab** 

### 0.â³æ—¥å¿—

<div align=center>

|æ—¶é—´ç‚¹|æäº¤å†…å®¹|è¯´æ˜|
|-|-|-|
|2023-08-21|å’ŒNVIDIAå¯¼å¸ˆå›¢é˜Ÿç¡®å®šä¼˜åŒ–æ–¹æ¡ˆï¼šåŸºäºå¼€æºLLMçš„LLaMAæ¨¡å‹æ¨æ–­åŠ é€Ÿä¼˜åŒ–|é€‰é¢˜|
|2023-08-22|åˆ›å»ºGithubé¡¹ç›®                                              |é¡¹ç›®åˆ›å»º|
|2023-08-24|å®Œæˆé€åˆ†é¢˜ä½œç­”                                               |é€åˆ†é¢˜ |
|2023-08-31|examples/llamaæºç å­¦ä¹                                        |é¡¹ç›®åˆ†æ |
|2023-09-03|æ­£å¸¸è¿è¡Œexamples/llama                                     |ä»£ç è¿è¡Œæµ‹è¯• |
|2023-09-09|examples/llama trickæ¶ˆèå®éªŒ                                |æ¶ˆèå®éªŒ |
|2023-09-17|examples/llama æ–°featureçš„å®ç°å’Œæµ‹è¯•                          |æ–°featureå®ç° |
|2023-09-20|READMEæŠ¥å‘Šä¹¦å†™å’Œä»£ç æ•´ç†æäº¤                                |æŠ¥å‘Šä¹¦å†™ |

â˜£ï¸å¤èµ›è°ƒä¼˜é˜¶æ®µï¼š2023å¹´8æœˆ17æ—¥-9æœˆ21æ—¥
</div>


### 1.æ€»è¿°
---

ä½œä¸º [NVIDIA TensorRT Hackathon 2023](https://github.com/NVIDIA/trt-samples-for-hackathon-cn/tree/master/Hackathon2023) çš„å¤èµ›å‚èµ›é¢˜ç›®ï¼Œæœ¬å·¥ä½œåŸºäºTensorRT-LLMä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹LLaMAã€‚æŒ‰ç…§å‚èµ›è¦æ±‚æœ¬é¡¹ç›®é€‰æ‹©å®Œæˆ**TensorRT-LLMè¯•ç”¨é€åˆ†é¢˜**ä»¥åŠ**3+4æ¨¡å¼çš„TensorRT-LLMæ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ**ï¼Œå³ï¼š
+ 3ï¼šç”¨TensorRT-LLMä¼˜åŒ–examplesç›®å½•ä¸‹çš„æŸä¸ªç°æœ‰æ¨¡å‹ï¼ˆæœ¬é¡¹ç›®ä¼˜åŒ–`examples/llama`æ¨¡å‹ï¼‰
+ 4ï¼šå°è¯•ä¸ºTensorRT-LLMæ·»åŠ æ–°featureï¼Œæˆ–è€…åœ¨æ¨¡å‹ä¸Šå¯ç”¨äº†ç°æœ‰feature

ä¸‹é¢æˆ‘ä»¬å°†ä»æ¨¡å‹ä»‹ç»ï¼Œä¼˜åŒ–æ•ˆæœåŠå¦‚ä½•è¿è¡Œè¯¥é¡¹ç›®ç­‰3ä¸ªæ–¹é¢ä»‹ç»æœ¬é¡¹ç›®

é¦–å…ˆæ˜¯LLaMAæ¨¡å‹çš„ç›¸å…³ä»‹ç»ï¼Œ[LLaMA](https://github.com/facebookresearch/llama) æ˜¯ç›®å‰ä¸ºæ­¢ï¼Œæ•ˆæœæœ€å¥½çš„å¼€æº LLM ä¹‹ä¸€,æ•°æ®é›†å±‚é¢ä¸Šå…±æœ‰1.4Tçš„Tokens, tokenizerä½¿ç”¨byte pair encoding (BPE) ç®—æ³•ï¼ŒSentence-Pieceçš„å®ç°,æ‰€æœ‰æ•°å­—è¢«æ‹†åˆ†ä¸ºå•ç‹¬çš„digitï¼Œæ‰€æœ‰æœªçŸ¥çš„UTF-8 å­—ç¬¦ï¼Œå›é€€åˆ°å­—èŠ‚æ¥è¿›è¡Œåˆ†è§£ã€‚å› æ­¤ï¼ŒLLaMA å¯ä»¥é€šè¿‡byte çš„æ–¹å¼ï¼Œæ„é€ å‡ºå¾ˆå¤šä¸åœ¨ vocab ä¸­çš„å­—ç¬¦ï¼Œä»è€Œä¹Ÿå…·æœ‰è¾ƒå¥½çš„å¤šè¯­è¨€èƒ½åŠ›ã€‚ç½‘ç»œç»“æ„ä¸Šçš„æ”¹è¿›åŸºäºTransformerçš„æ¶æ„ï¼Œå¹¶åšäº†å¦‚ä¸‹3ç‚¹æ”¹è¿›ï¼š
+ Pre-Normalizationï¼šä¸ºäº†æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¯¹æ¯ä¸ªtransformerå±‚çš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ï¼ˆä½¿ç”¨ RMS Norm å½’ä¸€åŒ–å‡½æ•°ï¼‰
+ SwiGLUï¼šä½¿ç”¨SwiGLUæ›¿ä»£äº†ReLUä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚å’ŒPaLMä¸­ä¸åŒï¼Œç»´åº¦é‡‡ç”¨$\frac{2}{3}4d$è€Œä¸æ˜¯$4d$  
+ RoPEï¼šé‡‡ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼Œä½¿å¾—å¤§æ¨¡å‹çš„ç”Ÿæˆæœ‰æ›´å¥½çš„å¤–æ¨æ€§

LLaMA-7Bæœ‰32ä¸ªè¿™æ ·çš„transformer blockæ„æˆï¼ŒLLaMA-13B ä¼˜äº GPT-3ï¼Œå°½ç®¡åªæœ‰1/10å¤§å°ã€‚ LLaMA-65B æ˜¯å¯ä»¥ä¸ Chinchilla-70B å’Œ PaLM-540B è¿™ç§æœ€ä½³çš„LLMç›¸ç«äº‰çš„æ¨¡å‹ã€‚ç»è¿‡å¾®è°ƒä¹‹åï¼ŒLLaMAçš„æ•ˆæœæœ‰æ˜¾è‘—çš„æå‡ã€‚å…³äºLLaMAçš„ä»‹ç»ï¼Œæ¨èçŸ¥ä¹æ–‡ç« ï¼š
+ [LLaMA è¶…è¯¦ç»†è§£è¯»ï¼ˆpaper & codeï¼‰](https://zhuanlan.zhihu.com/p/632102048?utm_id=0)

é’ˆå¯¹äºLLaMA-7Bå’ŒTensorRT-LLMä¸‹çš„`examples/llama`,æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹æ¡ˆè®¡åˆ’å®ç°è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<div align=center>
<img src="./assets/TensorRT-LLM LLaMaè¿›åº¦å›¾.png"/>
</div>

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒåŸºäº`examples/llama`æˆ‘ä»¬å®ç°äº†ï¼š
+ `examples/llama`ç°æœ‰featureçš„æ¶ˆèå®éªŒå¹¶åŸºäºsight Systemsè¿›è¡Œäº†Profiling
+ å®ç°äº†`examples/llama`æš‚æœªå®ç°çš„int8 k/v cacheå’Œsmoothquant
+ å¯¹å„ç§æƒ…å†µè¿›è¡Œå»¶æ—¶ï¼ŒåŠ é€Ÿæ¯”å’Œç²¾åº¦çš„å¯¹æ¯”
+ åˆèµ›é˜¶æ®µåœ¨TensorRTå®˜æ–¹Repoæäº¤äº†å…³äº†InstanceNorm Pluginçš„ä¸€ä¸ªbug
+ é€åˆ†é¢˜çš„ä½œç­”
+ æ—¶é—´å…³ç³»æˆªæ­¢é¡¹ç›®æäº¤smoothquantçš„erroré—®é¢˜å’Œinflght batchingæš‚æœªå®ç°

æœ€ç»ˆä¼˜åŒ–æ•ˆæœä¸ºï¼š
+ å°è¯•æ¶ˆèå„ç§featureæ™®éçš„TensorRT-LLMçš„åŠ é€Ÿæ¯”åœ¨`1.18-2.74`ä¹‹é—´
+ TensorRT-LLMå¯ä»¥å¾ˆå¥½çš„æŠŠrouge scoreçš„å·®å¼‚æ§åˆ¶åœ¨1ä»¥å†…æˆ–å·¦å³
+ æ–°feature int8 k/v cacheçš„æµ‹è¯•ç»“æœå¯ä»¥æ­£å¸¸å·¥ä½œä¸”æœ‰ä¸€å®šçš„åŠ é€Ÿæ•ˆæœ

è¯¦ç»†çš„ä¼˜åŒ–æ•ˆæœè¯·å‚è€ƒsection 2å’Œsection 3çš„ä»‹ç»ã€‚

è¯¥é¡¹ç›®æµ‹è¯•è¿è¡Œçš„è½¯ç¡¬ä»¶ç¯å¢ƒè¯·å‚è€ƒsection 3.1,ä¸‹é¢æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»å¦‚ä½•é€æ­¥è¿è¡Œè¯¥é¡¹ç›®:

+ é¡¹ç›®ç»“æ„

<details>
<summary>ç‚¹æˆ‘æŸ¥çœ‹é¡¹ç›®ç»“æ„</summary>

```shell
./tensorrt_llm_july-release-v1
â”œâ”€â”€ examples               # è¿™é‡Œå­˜æ”¾äº†äº†æˆ‘ä»¬çš„æ ¸å¿ƒä»£ç !
â”‚   â”œâ”€â”€ bert  
â”‚   â”œâ”€â”€ bloom
â”‚   â”œâ”€â”€ chatglm6b
â”‚   â”œâ”€â”€ cpp_library  
â”‚   â”œâ”€â”€ gpt               #é€åˆ†é¢˜
â”‚   â”œâ”€â”€ gptj
â”‚   â”œâ”€â”€ gptneox
â”‚   â”œâ”€â”€ lamma            # llamav1-7b featureæ¶ˆèå®éªŒ
â”‚    â”œâ”€â”€ build.py        # æ„å»ºengine
â”‚    â”œâ”€â”€ README.md       # readme
â”‚    â”œâ”€â”€ requirements.txt  # python package requirements
â”‚    â”œâ”€â”€ run.py          # run tensorrt-llm engine
â”‚    â”œâ”€â”€ run_hf.py       # run hf model 
â”‚    â”œâ”€â”€ summarize.py    # æ–‡æœ¬æ‘˜è¦æµ‹è¯•ä»»åŠ¡
â”‚    â””â”€â”€ weight.py       # build engineè¿‡ç¨‹ä¸­åŠ è½½hfæˆ–metaæƒé‡æ–‡ä»¶
|
â”‚   â”œâ”€â”€ llama_quant      # llama v1-7b æ–°featureçš„å®ç° 
â”‚    â”œâ”€â”€ build.py        # æ„å»ºengine æ”¯æŒint8 k/v cache, smooth quant
â”‚    â”œâ”€â”€ convert.py      # hfæ¨¡å‹è½¬ftæ¨¡å‹
â”‚    â”œâ”€â”€ hf_llama_convert.py # hfæ¨¡å‹è½¬ftæ¨¡å‹ å…¥å£
â”‚    â”œâ”€â”€ llama_model.py      # tensorrt-llm llama v1-7b æ¨¡å‹ç»“æ„
â”‚    â”œâ”€â”€ README.md       # readme
â”‚    â”œâ”€â”€ requirements.txt # python package requirements
â”‚    â”œâ”€â”€ run.py          # run tensorrt engine
â”‚    â”œâ”€â”€ run_hf.py       # run hf model
â”‚    â”œâ”€â”€ summarize.py    # æ–‡æœ¬æ‘˜è¦ä»»åŠ¡æµ‹è¯•
â”‚    â”œâ”€â”€ check_weight.py # æ£€æŸ¥hfæ¨¡å‹æƒé‡å’Œftæ¨¡å‹æƒé‡çš„ä¸€è‡´æ€§
â”‚    â”œâ”€â”€ quant.py        # smooth quant æ›¿æ¢æ–‡ä»¶
â”‚    â”œâ”€â”€ smoothquant.py  # smooth quantçš„å®ç°
â”‚    â””â”€â”€ weight_quant.py # build engineè¿‡ç¨‹ä¸­åŠ è½½ft weightæ”¯æŒint8 k/v cache,smoothquant
|
â”‚   â”œâ”€â”€ openai_triton      
â”‚   â””â”€â”€ opt  
â”‚ 
â”œâ”€â”€ ... # tensorrt_llm_july-release-v1ä¸­çš„å…¶ä»–åº“æ–‡ä»¶æˆ–ä»£ç 
â”‚ 
â””â”€â”€ README.md  # tensorrt_llm_july-release-v1å†…çš„readme
```

</details>

+ é€æ­¥è¿è¡Œæ–¹å¼è¯´æ˜

<details>
<summary>ç‚¹æˆ‘æŸ¥çœ‹è¿è¡Œæ–¹å¼</summary>

1.ç¯å¢ƒå‡†å¤‡

```shell
# æ‹‰å–é•œåƒ 
docker pull registry.cn-hangzhou.aliyuncs.com/trt-hackathon/trt-hackathon:final_v1

# run é•œåƒ
nvidia-docker run -it --name trt2023 registry.cn-hangzhou.aliyuncs.com/trt-hackathon/trt-hackathon:final_v1

# è¿›å…¥å®¹å™¨ä¸­æ–°å»ºä¸´æ—¶æ–‡ä»¶å¤¹,å¹¶cloneæœ¬é¡¹ç›®
mkdir temp
cd temp
git clone https://github.com/TRT2022/trtllm-llama

# è¿›å…¥cloneçš„é¡¹ç›®
cd trtllm_llama/tensorrt_llm_july-release-v1/examples

# å°†examplesä¸­çš„llamaå’Œllama_quant copyæ›¿æ¢é•œåƒä¸­çš„é¡¹ç›®
cp -r ./llama /root/workspace/tensorrt_llm_july-release-v1/examples/
cp -r ./llama_quant /root/workspace/tensorrt_llm_july-release-v1/examples/

# å°†llama_quant/quant.pyæ–‡ä»¶copyææ›¿æ¢åˆ°python package
cd /root/workspace/tensorrt_llm_july-release-v1/examples/llama_quant
cp ./quant.py /usr/local/lib/python3.8/dist-packages/tensorrt_llm/models/quantized/

# å®‰è£…å¿…è¦çš„python package
pip3 install -r requirements.txt -i  http://mirrors.aliyun.com/pypi/simple/

cd /root/workspace/tensorrt_llm_july-release-v1/examples
```

2.é€åˆ†é¢˜è¯¦ç»†è§section5

3.å°è¯•è¿è¡Œllama

+ ä¸‹è½½æ¨¡å‹

LlaMA-7B v1 (meta checkpoint)æ¨¡å‹ä¸‹è½½åœ°å€ï¼š <https://115.com/s/sw6a2kv3w4z?password=a835&#>,å°†ä¸‹è½½åçš„æ¨¡å‹å­˜æ”¾åœ¨`/tensorrt_llm_july-release-v1/examples/llama/llama-1-7b-meta/`ä¸‹

+ meta checkpoint è½¬ huggingface(ä»¥ä¸‹ç®€ç§°HF) checkpoint

```shell
# cdåˆ°ç›®æ ‡è·¯å¾„
cd ./tensorrt_llm_july-release-v1/examples/llama
# æ¨¡å‹è½¬HF checkpoint
python3 /usr/local/lib/python3.8/dist-packages/transformers/models/llama/convert_llama_weights_to_hf.py  --input_dir ./llama-1-7b-meta --model_size 7B --output_dir ./tmp/llama/7B
```

+ æ„å»ºTensorRT-LLM engine

```shell
# åŠ å…¥plugin
python build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu/
```

+ è¿è¡Œengine

```shell
python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/
```

+ ä½¿ç”¨LLaMA-7Bæµ‹è¯•æ–‡æœ¬æ‘˜è¦ä»»åŠ¡

```shell
# ä½¿ç”¨TensorRT-LLM engineæµ‹è¯•
python3 summarize.py --test_trt_llm \
                    --hf_model_location ./tmp/llama/7B/ \
                    --data_type fp16 \
                    --engine_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu/

# ä½¿ç”¨HFæ¨¡å‹æµ‹è¯•
python3 summarize.py --test_hf \
                    --hf_model_location ./tmp/llama/7B/ \
                    --data_type fp16
```

4.llama featureæ¶ˆèå®éªŒ

```shell
cd /root/workspace/tensorrt_llm_july-release-v1/examples/llama

# ------------k/vcache+attention plugin---------------
# build engine
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu \
# run engine
run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/

# run HF checkpoint
python3 run_hf.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --hf_model_location ./tmp/llama/7B/

# ----------k/v cache + attention_plugin + weight_only_quant---------
# build engine
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_weight_only \
                --output_dir ./tmp/llama/7B/trt_engines/int8_kvcache/1-gpu/

#run engine
python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/weight_only/1-gpu/

#----k/v cache + attention plugin + weight_only_quant + gemm plugin-----
# build engine
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --use_weight_only \
                --output_dir ./tmp/llama/7B/trt_engines/weight_only_attention_gemm/1-gpu/

#run engine
python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/weight_only_attention_gemm/1-gpu/

#-----------int4 weight only quant--------------
# build engine
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --use_weight_only \
                --weight_only_precision int4 \
                --output_dir ./tmp/llama/7B/trt_engines/int4/1-gpu/

#run engine
nsys profile -o trt_llm__int4 python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/int4/1-gpu/
```

5.llama æ–°featureçš„å®ç°

```shell
cd /root/workspace/tensorrt_llm_july-release-v1/examples/llama_quant

#-----------int8 k/v cache---------------
# HF model è½¬ FT model support int8 k/v cache
python3 hf_llama_convert.py -i tmp/llama/7B \
                            -o ./c-model/llama \
                            --tensor-parallelism 1  \
                            --storage-type float16  \
                             --calibrate-kv-cache 
# build int8 k/v cache engine
python3 build.py --model_dir=./c-model/llama/1-gpu \
                 --use_gpt_attention_plugin float16\
                 --int8_kv_cache \
                 --output_dir=./tmp/llama/7B/trt_engines/int8kv/1-gpu/

# run int8 k/v cache engine
python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/int8kv/1-gpu/

# summary data test engine
python3 summarize.py --test_trt_llm  \
                     --hf_model_location ./tmp/llama/7B/  \
                     --data_type fp16 \
                     --engine_dir ./tmp/llama/7B/trt_engines/int8kv/1-gpu/

```

```shell
cd /root/workspace/tensorrt_llm_july-release-v1/examples/llama_quant
#---------------smooth quant---------------

# hf model è½¬ ft model support smooth quant
python3 hf_llama_convert.py -i tmp/llama/7B \
                            -o ./c-model/llama \
                            --tensor-parallelism 1   \
                            --storage-type float16  \
                            --smoothquant 0.5

# build smooth quant model
python3 build.py --model_dir=./c-model/llama/1-gpu \
                 --use_gpt_attention_plugin float16\
                 --use_smooth_quant \
                 --output_dir=./tmp/llama/7B/trt_engines/sm/1-gpu/

```

</details>


### 2.ä¸»è¦å¼€å‘å·¥ä½œ
---

#### 2.1 å¼€å‘å·¥ä½œçš„éš¾ç‚¹

è¯¥æ¨¡å‹çš„ä¼˜åŒ–éš¾ç‚¹å¦‚ä¸‹ï¼š

+ å¯¹äºTensorRT-LLMç›¸å…³åŠŸèƒ½å’ŒAPIä¸ç†Ÿæ‚‰
+ `examples/llama`ä¸­å·²ç»å®ç°äº†å¤šæ•°çš„featureï¼Œåœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥çš„ä¼˜åŒ–éš¾åº¦è¾ƒå¤§

ä½†æ˜¯ï¼ŒLLaMAä½œä¸ºç›¸å¯¹è¾ƒæ—©çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œå…¶ç›´æ¥å½±å“äº†å›½å†…å¤–å¤§æ¨¡å‹çš„å‘å±•å’Œç ”å‘æ€è·¯ï¼Œå…¶é‡è¦æ€§ä¸è¨€è€Œå–»ï¼Œé’ˆå¯¹äºLLaMAçš„TensorRT-LLMæ¨¡å‹ä¼˜åŒ–æ„ä¹‰é‡å¤§ã€‚

#### 2.2 å¼€å‘ä¸ä¼˜åŒ–è¿‡ç¨‹

é’ˆå¯¹äºLLaMA-7Bæˆ‘ä»¬çš„ä¼˜åŒ–è¿‡ç¨‹ä¸»è¦åˆ†ä»¥ä¸‹3ä¸ªéƒ¨åˆ†ï¼š
+ 1.åˆæ­¥è¿è¡Œ`examples/llama`é¡¹ç›®
+ 2.nsight systermåˆ†æé€æ­¥æ·»åŠ featureè¿›è¡Œæ¶ˆèå®éªŒ
+ 3.æ–°featuteå®ç°ï¼šint8 k/v cache, smoothquantå’Œinflight batching

æ¯ä¸€éƒ¨åˆ†æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„è¿è¡Œè„šæœ¬å’Œæµ‹è¯•ç»“æœã€‚

##### 2.2.1 åˆæ­¥è¿è¡Œ`examples/llama`é¡¹ç›®

0. å‡†å¤‡LLaMA-7B meta checkpointæ¨¡å‹

+ ä¸‹è½½æ¨¡å‹

LlaMA-7B v1 (meta checkpoint)æ¨¡å‹ä¸‹è½½åœ°å€ï¼š <https://115.com/s/sw6a2kv3w4z?password=a835&#>,å°†ä¸‹è½½åçš„æ¨¡å‹å­˜æ”¾åœ¨`/tensorrt_llm_july-release-v1/examples/llama/llama-1-7b-meta/`ä¸‹

+ meta checkpoint è½¬ huggingface(ä»¥ä¸‹ç®€ç§°HF) checkpoint

```shell
# cdåˆ°ç›®æ ‡è·¯å¾„
cd ./tensorrt_llm_july-release-v1/examples/llama
# æ¨¡å‹è½¬HF checkpoint
python3 /usr/local/lib/python3.8/dist-packages/transformers/models/llama/convert_llama_weights_to_hf.py  --input_dir ./llama-1-7b-meta --model_size 7B --output_dir ./tmp/llama/7B
```

1. æ„å»ºTensorRT-LLM engine

```shell
# ä¸åŠ å…¥ä»»ä½•trick
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu \
                --visualize

# åŠ å…¥plugin
python build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu/
```

2. è¿è¡Œengine

```shell
python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/
```

3. ä½¿ç”¨LLaMA-7Bæµ‹è¯•æ–‡æœ¬æ‘˜è¦ä»»åŠ¡

```shell
# ä½¿ç”¨TensorRT-LLM engineæµ‹è¯•
python3 summarize.py --test_trt_llm \
                    --hf_model_location ./tmp/llama/7B/ \
                    --data_type fp16 \
                    --engine_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu/
```
ç»“æœï¼š

```
[09/03/2023-13:56:21] [TRT-LLM] [I] ---------------------------------------------------------
[09/03/2023-13:57:32] [TRT-LLM] [I] TensorRT-LLM (total latency: 65.88509821891785 sec)
[09/03/2023-13:57:32] [TRT-LLM] [I] TensorRT-LLM beam 0 result
[09/03/2023-13:57:32] [TRT-LLM] [I]   rouge1 : 19.478572394974464
[09/03/2023-13:57:32] [TRT-LLM] [I]   rouge2 : 5.748473587185184
[09/03/2023-13:57:32] [TRT-LLM] [I]   rougeL : 14.488586709461371
[09/03/2023-13:57:32] [TRT-LLM] [I]   rougeLsum : 17.818188740969955
```

```shell
# ä½¿ç”¨HFæ¨¡å‹æµ‹è¯•
python3 summarize.py --test_hf \
                    --hf_model_location ./tmp/llama/7B/ \
                    --data_type fp16 
```

ç»“æœï¼š

```
[09/03/2023-14:02:07] [TRT-LLM] [I] ---------------------------------------------------------
[09/03/2023-14:03:30] [TRT-LLM] [I] Hugging Face (total latency: 78.22841620445251 sec)
[09/03/2023-14:03:30] [TRT-LLM] [I] HF beam 0 result
[09/03/2023-14:03:30] [TRT-LLM] [I]   rouge1 : 20.106338916310662
[09/03/2023-14:03:30] [TRT-LLM] [I]   rouge2 : 5.910110463256421
[09/03/2023-14:03:30] [TRT-LLM] [I]   rougeL : 15.2269090887293
[09/03/2023-14:03:30] [TRT-LLM] [I]   rougeLsum : 17.938095329383458
```

æˆ‘ä»¬åˆæ­¥æ­£å¸¸è¿è¡Œäº†`example/llama`,ä»ç»“æœä¸Šåˆæ­¥éªŒè¯äº†æ­£ç¡®æ€§ã€‚

##### 2.2.2 nsight systemåˆ†æé€æ­¥æ·»åŠ featureè¿›è¡Œæ¶ˆèå®éªŒ

è¯¥éƒ¨åˆ†æˆ‘ä»¬åšäº†è¯¦ç»†çš„æ¶ˆèå®éªŒï¼Œé€šè¿‡é€æ­¥æ·»åŠ featureå’Œtrickçš„æ–¹å¼éªŒè¯ä¸åŒfeatureåœ¨LLaMA-7Bä¸Šçš„Latencyçš„æ”¶ç›Šï¼Œå¹¶åŸºäºnsight systemè¿›è¡Œprofilingã€‚

æˆªæ­¢æœ¬é¡¹ç›®å¤èµ›æä¾›çš„é•œåƒ`examples/llama`çš„featureæ”¯æŒæƒ…å†µå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<div align=center>
<img src="./assets/TensorRT_LLM LLaMa.png"/>
</div>

ç”±äºLLaMA-7Bä¸­ä½¿ç”¨äº†RoPE,ç›®å‰`gpt_attention_plugin`(ä»¥ä¸‹ç®€ç§°attention plugin)æ˜¯ç›®å‰å”¯ä¸€çš„ä¸€ç§æ”¯æŒRoPEçš„æ–¹å¼ï¼Œå› æ­¤LLaMAåœ¨TensorRT-LLMä¸­å¼ºåˆ¶ä½¿ç”¨äº†`gpt_attention_plugin`

1. æ·»åŠ : k/v cache + attention pligin

+ build engine

```shell
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu \
```

+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
# è¿è¡Œengine
nsys profile -o trt_llm_only_kv_cache_fp16 python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/

# è¿è¡ŒHF checkpoint
python3 run_hf.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --hf_model_location ./tmp/llama/7B/
```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 1.404158673286438 sec)
# HF
llama-hf-run (mean latency: 1.7185083055496215 sec)
```
ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `k/v cache + attention plugin`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.40416ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`1.224`

åˆ†æå¯¼å‡ºçš„`trt_llm_only_kv_cache_fp16` nsysæ–‡ä»¶ï¼Œå¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°attention pluginå’Œk/v cacheä»¥åŠçŸ©é˜µä¹˜çš„æ¨ç†å»¶æ—¶æƒ…å†µå¦‚ä¸‹æ‰€ç¤ºï¼š

+ attention plugin profilingçš„è€—æ—¶æƒ…å†µ
<div align=center>
<img src="./assets/kv_cache_fp16/attention_1.png"/>
</div>

+ k/v cache profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/kv_cache_fp16/kvcache.png"/>
</div>

+ å¸¦æƒé‡çš„çŸ©é˜µä¹˜çš„profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/kv_cache_fp16/matrix_multiply_0.png"/>
</div>

å¯ä»¥çœ‹åˆ°åœ¨FP16ä¸‹ï¼Œattention pluginçš„latencyä¸º$13.576\mu s$,k/v cacheçš„latencyä¸º$925.928\mu s$,å¸¦æƒé‡çš„çŸ©é˜µä¹˜çš„latencyä¸º$45.922\mu s$

2. æ·»åŠ : k/v cache + attention_plugin + weight_only_quant

+ build engine

```shell
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_weight_only \
                --output_dir ./tmp/llama/7B/trt_engines/int8_kvcache/1-gpu/
```

+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
nsys profile -o trt_llm_weight_only python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/weight_only/1-gpu/
```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 0.7849386262893677 sec)
```
ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `k/v cache + attention plugin + weight_only_quant`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`0.78494ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`2.189`

åˆ†æ`trt_llm_weight_only`nsysæ–‡ä»¶ï¼Œå¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°attention pluginå’ŒçŸ©é˜µä¹˜çš„æ¨ç†å»¶æ—¶æƒ…å†µå¦‚ä¸‹æ‰€ç¤ºï¼š

+ attention plugin profilingçš„è€—æ—¶æƒ…å†µ
<div align=center>
<img src="./assets/weight_only_quant/attention.png"/>
</div>

+ å¸¦æƒé‡çš„çŸ©é˜µä¹˜çš„profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/weight_only_quant/matmul.png"/>
</div>

å¯ä»¥çœ‹åˆ°åœ¨weight only quantä¸‹ï¼Œattention pluginçš„latencyä¸º$17.837\mu s$,å¸¦æƒé‡çš„çŸ©é˜µä¹˜çš„latencyä¸º$7.107\mu s$ã€‚å¯¹æ¯”ä¸Šè¿°FP16çš„æƒ…å†µæœ‰æ˜æ˜¾çš„åŠ é€Ÿæ•ˆæœã€‚


3. æ·»åŠ : k/v cache + attention plugin + weight_only_quant + gemm plugin

+ build engine

```shell
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --use_weight_only \
                --output_dir ./tmp/llama/7B/trt_engines/weight_only_attention_gemm/1-gpu/

```
+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
nsys profile -o trt_llm_weight_only_attention_gemm python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/weight_only_attention_gemm/1-gpu/

```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 0.7930449199676514 sec)
```
ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `k/v cache + attention plugin + weight_only_quant + gemm plugin`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`0.79304ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`2.167`

åˆ†æ`trt_llm_weight_only_attention_gemm`nsysæ–‡ä»¶ï¼Œå¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°gemmåœ¨ä½¿ç”¨pluginå‰åçš„æ¨ç†å»¶æ—¶æƒ…å†µå¦‚ä¸‹æ‰€ç¤ºï¼š

+ gemm pluginå‰ profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/gemm/gemm_origin_5.png"/>
</div>

+  gemm pluginå‰ profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/gemm/gemm_5.png"/>
</div>

å¯ä»¥æ˜æ˜¾çœ‹åˆ°æ›¿æ¢gemm pluginçš„å‰åå˜åŒ–ï¼Œgemm pluginæ›¿æ¢å‰çš„latencyä¸º$28.286\mu s$,gemm pluginæ›¿æ¢åçš„latencyä¸º$26.241\mu s$,æœ‰ä¸€å®šçš„åŠ é€Ÿæ•ˆæœã€‚

4. int4 weight only quant

+ build engine

```shell
python3 build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --use_gpt_attention_plugin float16 \
                --use_gemm_plugin float16 \
                --use_weight_only \
                --weight_only_precision int4 \
                --output_dir ./tmp/llama/7B/trt_engines/int4/1-gpu/

```
+ nsight system profilingåŠlatencyç»Ÿè®¡

```shell
nsys profile -o trt_llm__int4 python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/int4/1-gpu/
```

å¾—åˆ°ç»“æœï¼š

```
# TensorRT-LLM 
llama-run (mean latency: 0.48769086837768555 sec)
```

ä¸Šè¿°ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ `int4`åçš„TensorRT LLaMAçš„å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`0.48769ç§’`ï¼Œè€ŒHFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`,åŠ é€Ÿæ¯”ä¸º`3.524`

åˆ†æ`trt_llm__int4`nsysæ–‡ä»¶ï¼Œå¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°attention pluginå’Œk/v cacheæ¨ç†å»¶æ—¶æƒ…å†µå¦‚ä¸‹æ‰€ç¤ºï¼š

+ attention plugin profilingçš„è€—æ—¶æƒ…å†µ
<div align=center>
<img src="./assets/int4/attention.png"/>
</div>

+ k/v cache profilingçš„è€—æ—¶æƒ…å†µ

<div align=center>
<img src="./assets/int4/kvcache.png"/>
</div>

å¯ä»¥çœ‹åˆ°åœ¨int4ä¸‹ï¼Œattention pluginçš„latencyä¸º$11.609\mu s$,K/V cacheçš„latencyä¸º$159.717\mu s$

ç»¼ä¸ŠåŸºäºä¸Šè¿°åˆ†æç»“æœï¼Œæ€»ç»“å¦‚ä¸‹ï¼š
<div align=center>
<p>Table-1: Featureä¸Latencyçš„æ¶ˆèå®éªŒ(examples/llamaç°æ”¯æŒçš„feature)</p>

|Feature|åŸLlamaæ˜¯å¦å®ç°|æœ¬é¡¹ç›®æ˜¯å¦å¯ç”¨|batch size|input length|output length|åŠ é€Ÿæ¯”|
|-|-|-|-|-|-|-|
| K/V cache|âœ”ï¸|âœ”ï¸|1|8|50|-|
|+Attention Plugin|âœ”ï¸|âœ”ï¸|1|8|50|1.224|
|+Weight Only Quant|âœ”ï¸|âœ”ï¸|1|8|50|2.189|
|+Gemm Plugin|âœ”ï¸|âœ”ï¸|1|8|50|2.167|
|Int4 Weight Only Quant|âœ”ï¸|âœ”ï¸|1|8|50|3.524|
|Int8 K/V cache|âŒ|-|1|8|50|-|
|SmoothQuant|âŒ|-|1|8|50|-|
|Inflight Batching|âŒ|-|1|8|50|-|

</div>

âš ï¸æ³¨æ„ï¼šæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚æ–°featureçš„å®ç°ä¸­è¿›ä¸€æ­¥å®Œå–„è¯¥è¡¨æ ¼ï¼Œå¹¶åœ¨Section3-ä¼˜åŒ–æ•ˆæœçš„ç¬¬ä¸€éƒ¨åˆ†æä¾›ç°æœ‰featureä¸‹çš„åŠ é€Ÿæ•ˆæœå’Œç²¾åº¦å¯¹æ¯”ã€‚

#### 2.2.3 æ–°featuteå®ç°ï¼šint8 k/v cache,smoothquantï¼Œinflight batching

ä¸ºäº†ä½“ç°æˆ‘ä»¬åœ¨æœ¬é¡¹ç›®çš„å·¥ä½œï¼Œæˆ‘ä»¬å°†æ–°featureçš„å®ç°å•ç‹¬åœ¨`examples`ä¸­æ„å»ºäº†ä¸€ä¸ªæ–°çš„é¡¹ç›®`examples/llama_quant`

1.int8 k/v cache

<div align=center>
<img src="./assets/int8_kv_cache.png"/>
</div>

int8 k/v cacheæœ¬è´¨å’Œweight only quantä¸€æ ·ï¼Œåœ¨æ¨¡å‹generation phaseè¯»å–ä¹‹å‰çš„Kå’ŒVç±»ä¼¼äºweight only quantä¸­å…¨è¿æ¥å±‚ä¸­è¯»å–weight,å‚ç…§weight only quantä¹ŸæŠŠK,Vç”¨int8ä¿å­˜ä¸‹æ¥ï¼Œå½“æˆ‘ä»¬çœŸæ­£è®¡ç®—çš„æ—¶å€™å†å°†å…¶dequantåˆ°é«˜ç²¾åº¦ã€‚

`examples/llama`æš‚æ—¶ä¸æ”¯æŒint8 k/v cache,è¿™é‡Œæˆ‘ä»¬å®ç°äº†`examples/llama`çš„int8 k/v cache


+ å°†HFæ¨¡å‹è½¬æ¢ä¸ºFasterTransformer(ä»¥ä¸‹ç®€ç§°FT)æ¨¡å‹

è¿™é‡Œæˆ‘å®ç°äº†LLaMA-7Bçš„HFæ¨¡å‹è½¬æ¢ä¸ºFTæ¨¡å‹ï¼Œè¿‡ç¨‹ä¸­å®ç°äº†int8 k/v cache.

```shell
cd tensorrt_llm_july-release-v1/examples/llama_quant

python3 hf_llama_convert.py -i tmp/llama/7B \
                            -o ./c-model/llama \
                            --tensor-parallelism 1  \
                            --storage-type float16  \
                             --calibrate-kv-cache 
```
è¾“å‡ºå¦‚ä¸‹ä¿¡æ¯ï¼Œæ”¯æŒint8 k/v cacheçš„FTæ¨¡å‹æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼Œç”Ÿæˆåçš„FTæ¨¡å‹æ–‡ä»¶å­˜æ”¾åœ¨å½“å‰è·¯å¾„ä¸‹çš„`c-model`æ–‡ä»¶å¤¹ä¸‹ï¼š

<div align=center>
<img src="./assets/int8kv_1.png"/>
</div>


+ æ„å»ºåŒ…å«int8 k/v cacheçš„engine

è¿™é‡Œæˆ‘ä»¬é‡å†™äº†`examples/llama`ä¸­çš„`build.py`,ä½¿å…¶å¯ä»¥æ”¯æŒint8 k/v cache

```shell
cd tensorrt_llm_july-release-v1/examples/llama_quant

python3 build.py --model_dir=./c-model/llama/1-gpu \
                 --use_gpt_attention_plugin float16\
                 --int8_kv_cache \
                 --output_dir=./tmp/llama/7B/trt_engines/int8kv/1-gpu/
```

è¾“å‡ºå¦‚ä¸‹ä¿¡æ¯ï¼Œæ”¯æŒint8 k/v cacheçš„engineåºåˆ—åŒ–å®Œæˆï¼Œå¹¶ä¿å­˜åœ¨`./tmp/llama/7B/trt_engines/int8kv`ä¸‹ï¼š

<div align=center>
<img src="./assets/int8kv_2.png"/>
</div>

+ Latencyçš„æµ‹è¯•

```shell
cd tensorrt_llm_july-release-v1/examples/llama_quant
python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/int8kv/1-gpu/
```

å…¶è¾“å‡ºç»“æœä¸ºï¼š

```
# int8 k/v cache engine
llama-run (mean latency: 1.4051964092254638 sec)
```
`int8 k/v cache + attention plugin`çš„å¹³å‡æ¨ç†å»¶æ—¶ä¸º`1.40519ç§’`ï¼Œä¸Šæ–‡å¯çŸ¥HFä¸‹å¹³å‡æ¨æ–­å»¶æ—¶ä¸º`1.71851ç§’`ï¼Œå…¶åŠ é€Ÿæ¯”ä¸ºï¼š`1.223`

+ åœ¨æ–‡æœ¬æ‘˜è¦æ•°æ®çš„Latencyå’Œç²¾åº¦

```shell
cd tensorrt_llm_july-release-v1/examples/llama_quant
python3 summarize.py --test_trt_llm  \
                     --hf_model_location ./tmp/llama/7B/  \
                     --data_type fp16 \
                     --engine_dir ./tmp/llama/7B/trt_engines/int8kv/1-gpu/
```
è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
[09/16/2023-12:03:17] [TRT-LLM] [I] ---------------------------------------------------------
[09/16/2023-12:04:28] [TRT-LLM] [I] TensorRT-LLM (total latency: 66.31966018676758 sec)
[09/16/2023-12:04:28] [TRT-LLM] [I] TensorRT-LLM beam 0 result
[09/16/2023-12:04:28] [TRT-LLM] [I]   rouge1 : 18.630357255367613
[09/16/2023-12:04:28] [TRT-LLM] [I]   rouge2 : 5.62976959191806
[09/16/2023-12:04:28] [TRT-LLM] [I]   rougeL : 14.616061481091847
[09/16/2023-12:04:28] [TRT-LLM] [I]   rougeLsum : 16.930935356053094
root@0933517de088:~/workspace/tensorrt_llm_july-release-v1/examples/llama_quant#
```

2.smoothquant

<div align=center>
<img src="./assets/smoothquant1.png"/>
</div>

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒæŸäº›LLMçš„æŸäº›channelæˆ–æŸäº›ç»´åº¦çš„activation outlierå€¼å¾ˆå¤šä¸”å¾ˆå¤§(ep. GLM-130Bæœ‰30%ï¼‰ï¼Œå¯¼è‡´é‡åŒ–çš„æœ‰æ•ˆä½å˜å°‘ï¼Œæ¯”å¦‚int8æœ¬æ¥æ˜¯-128åˆ°127ï¼Œæ²¡æœ‰outlierçš„æ—¶å€™ï¼Œæ˜ å°„åˆ°-128åˆ°127çš„æ•°æ®åˆ†å¸ƒå‡åŒ€ï¼Œå æ»¡äº†8bitä½èŒƒå›´ï¼Œç²¾åº¦æŸå¤±å¾ˆä½ï¼Œä½†æ˜¯æœ‰äº†outlierä¹‹åï¼Œå¤šæ•°æ­£å¸¸å€¼çš„åˆ†å¸ƒåŒºé—´å¯èƒ½åœ¨[-20,20]æˆ–è€…[-10,10]ï¼Œ8bitä½èŒƒå›´åªåˆ©ç”¨åˆ°äº†5bitï¼Œç”šè‡³4bitï¼Œç”±æ­¤å¯¼è‡´ç²¾åº¦æŸå¤±ã€‚ä¸Šå›¾ä¸­çš„activationçº¢è‰²æ©™è‰²æ˜¯outlierï¼Œoutlierä¸€èˆ¬é›†ä¸­å­˜åœ¨äºæŸå‡ ä¸ªchannelæˆ–axis,ä¸”activationæ¯”weightæ›´éš¾é‡åŒ–ï¼Œåè€…æ•°æ®åˆ†å¸ƒä¸€èˆ¬æ¯”è¾ƒå‡åŒ€ï¼Œsmoothquantçš„keypointsæ˜¯å¯ä»¥æŠŠactivationé‡åŒ–éš¾åº¦è¿ç§»åˆ°weightä¸Šæ¥ï¼ŒæŠŠactivationé‡Œé¢ä¸å‡åŒ€çš„åˆ†å¸ƒç”¨weightä¸­å’Œä¸€ä¸‹ï¼Œå…·ä½“æ¥è®²ï¼Œä¸»è¦æ˜¯åœ¨fp32é˜¶æ®µåšçš„ï¼Œä¿è¯ä¸€ä¸ªç­‰å¼çš„ç­‰ä»·ï¼Œ$X$ä¸ºè¾“å…¥activationï¼Œ$W$ä¸ºweightï¼Œ$s$ä¸ºå› å­ï¼Œé€šè¿‡$s$æ¥ä¸­å’Œ
$$Y=(Xdiag(s)^{-1}.(diag(s)W))=\hat{X}\hat{W}$$
ç›´è§‚çš„ç†è§£å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
<div align=center>
<img src="./assets/smoothquant2.png"/>
</div>

å·¦è¾¹ä¸ºsmoothå‰ï¼Œå³è¾¹ä¸ºsmoothåï¼Œå¯ä»¥æ˜æ˜¾çœ‹åˆ°Xä¹˜ä»¥$s^{-1}$ä¹‹åæ•°æ®åˆ†å¸ƒæ˜æ˜¾å‡åŒ€äº†ï¼ŒæŠŠéš¾åº¦åŒ€äº†ä¸€ç‚¹ç»™weightã€‚

`examples/llama`æš‚ä¸æ”¯æŒsmoothquantï¼Œè¿™é‡Œæˆ‘ä»¬å®ç°äº†`examples/llama`çš„smoothquantï¼Œå¹¶å°†å…¶å­˜æ”¾åœ¨`examples/llama_quant`é¡¹ç›®ä¸­

æœ¬èŠ‚ä¸­æˆ‘ä»¬é¦–é€‰å®ç°äº†LLaMA-7Bçš„HFè½¬FTæ¨¡å‹å¹¶ä»¥æ­¤å®ç°smoothquant,ä½†åœ¨è¿›è¡Œbuild engineçš„è¿‡ç¨‹ä¸­å‡ºç°äº†errorï¼Œæ—¶é—´å…³ç³»æš‚æœªåˆ†æå‡ºerroræ˜¯bugè¿˜æ˜¯å…¶ä»–ä»£ç åŸå› å¯¼è‡´ã€‚

+ ç”ŸæˆåŒ…å«smoothquantçš„FTæ¨¡å‹

```shell
cd tensorrt_llm_july-release-v1/examples/llama_quant
python3 hf_llama_convert.py -i tmp/llama/7B \
                            -o ./c-model/llama \
                            --tensor-parallelism 1   \
                            --storage-type float16  \
                            --smoothquant 0.5
```
è¿è¡Œä¸Šè¿°ä»£ç è¾“å‡ºå¦‚ä¸‹ï¼Œæˆ‘ä»¬æˆåŠŸçš„å°†HFæ¨¡å‹è½¬æ¢ä¸ºåŒ…å«smoothquantçš„FTæ¨¡å‹ï¼š
<div align=center>
<img src="./assets/smoothquant_1.png"/>
</div>

FTæƒé‡æ–‡ä»¶è¢«å­˜æ”¾åœ¨`c-model`ä¸‹ï¼Œå…¶ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<div align=center>
<img src="./assets/smoothquant_2.png"/>
</div>


+ æ„å»ºæ”¯æŒsmooth quantçš„engine

```shell
cd tensorrt_llm_july-release-v1/examples/llama_quant
python3 build.py --model_dir=./c-model/llama/1-gpu \
                 --use_gpt_attention_plugin float16\
                 --use_smooth_quant \
                 --output_dir=./tmp/llama/7B/trt_engines/sm/1-gpu/

```

åºåˆ—åŒ–engineè¿‡ç¨‹ä¸­ï¼ŒæŠ¥å¦‚ä¸‹é”™è¯¯ï¼š


<div align=center>
<img src="./assets/smooth_error.jpg"/>
</div>

ç”±äºæ—¶é—´å…³ç³»ï¼Œæˆ‘ä»¬å°è¯•è§£å†³è¯¥errorä½†æš‚æ—¶æ²¡æœ‰è§£å†³ï¼Œæˆ‘ä»¬ç›®å‰æ— æ³•ç¡®å®šä¸Šè¿°erroræ˜¯å¦æ˜¯TensorRT-LLMçš„bugã€‚


<!-- ```shell
python3 run.py --max_output_len=50 \
               --tokenizer_dir ./tmp/llama/7B/ \
               --engine_dir=./tmp/llama/7B/trt_engines/sm/1-gpu/
``` -->
> è¯¥é—®é¢˜æˆ‘ä»¬å°†åœ¨æ¯”èµ›ç»“æŸåç»§ç»­è§£å†³ï¼


3.inflight batching (#ToDo)

<div align=center>
<img src="./assets/inflight-batch.png"/>
</div>

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä¸ºäº†å¢åŠ thoughputæˆ‘ä»¬å¸Œæœ›æ¯æ¬¡æ¨æ–­è¿›batchçš„æ•°æ®ï¼Œå¯¹äºLLMæ¥è¯´é¦–å…ˆéœ€è¦ä¸€ä¸ªbatchçš„æ•°æ®ä¸­é•¿åº¦ä¸åŒçš„sequenceè¿›è¡Œpaddingåˆ°ç›¸åŒçš„é•¿åº¦ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºbatch size=3,é»‘è‰²çš„çŸ©å½¢æ¡†è¡¨ç¤ºå¯¹æ¯ä¸ªsequenceè¿›è¡Œçš„padding,å¦‚æœä¸è¿›è¡Œinflight batchingæ“ä½œï¼Œä¸€ä¸ªbatchçš„æ•°æ®å¿…é¡»å…¨éƒ¨generateå®Œæˆæ‰èƒ½ä¸€èµ·è¿”å›ï¼Œè€Œè¯¥bacthä¸­å³ä½¿æœ‰æå‰generateå®Œæˆçš„sequenceä¹Ÿå¿…é¡»ç­‰å¾…ã€‚

inflight batchingçš„è¿‡ç¨‹å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œé¦–å…ˆæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªRequest Waiting Poolè¿™é‡Œå­˜æ”¾äº†æ‰€æœ‰çš„å¾…æ¨æ–­çš„sequenceï¼Œå‡è®¾æœ‰ä¸€ä¸ªbatchçš„æ•°æ®paddingåç»è¿‡context phaseè¿›è¡Œgeneration phaseï¼Œbatchä¸­çš„ç¬¬2ä¸ªæ•°æ®æå‰generateå®Œåå³åˆ»è¿”å›ç»“æœï¼Œæ­¤æ—¶å¯ä»¥åœ¨Request Waiting Poolä¸­å–å‡ºè“è‰²çš„æ–°sequenceåŠ å…¥åˆ°å½“å‰batchä¸­ï¼Œè“è‰²çš„sequenceæ‰§è¡Œcontext phaseè¿›è€Œæ‰§è¡Œgeneration phase,batchä¸­çš„å…¶ä»–æ•°æ®ç»§ç»­æ‰§è¡Œgeneration phaseã€‚é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œç›´åˆ°Poolä¸­æ— éœ€è¦æ¨æ–­çš„æ•°æ®ä¸ºæ­¢ã€‚

ç”±äºæ—¶é—´å…³ç³»ï¼Œæœ¬æ¯”èµ›æ²¡èƒ½å®ç°`examples/llama`é¡¹ç›®çš„inflight batchingã€‚æˆ‘ä»¬ç›¸ä¿¡å¤èµ›æ˜¯æˆ‘ä»¬å°è¯•ä½¿ç”¨TensorRT-LLMçš„å¼€å§‹ï¼Œæˆ‘ä»¬å°†åœ¨åæœŸæŒç»­å®Œæˆinflight batchingçš„å®ç°ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬åœ¨æ–°featureéƒ¨åˆ†çš„å·¥ä½œä¸»è¦åŒ…æ‹¬ï¼š

+ å®ç°äº†LLaMA-7B HFæ¨¡å‹è½¬FTæ¨¡å‹åŠé’ˆå¯¹äºFTæ¨¡å‹æƒé‡çš„engine build
+ åŸºäºè¯¥æ¨¡å‹è½¬æ¢å®ç°äº†int8 k/v cacheå¹¶æˆåŠŸåºåˆ—åŒ–engineå®Œæˆç²¾åº¦å’Œå»¶æ—¶çš„æµ‹è¯•
+ åŸºäºè¯¥æ¨¡å‹è½¬æ¢å®ç°äº†smoothquantçš„è½¬æ¢å’Œç›¸åº”çš„build engineå’Œç²¾åº¦å»¶æ—¶æµ‹è¯•çš„ä»£ç éƒ¨åˆ†

å¾…å®Œæˆçš„å·¥ä½œï¼š

+ smoothquantåœ¨build engineä¸­å‡ºç°å…³äº`Cutlass int8 gemm`çš„æŠ¥é”™ï¼Œè¯¥é”™è¯¯æš‚æ—¶æœªè¢«è§£å†³ï¼ŒåŒæ—¶åœ¨build engineä¸­æš‚æœªå®ç°`GateMLP`çš„smoothquantçš„æ ¸å¿ƒä»£ç 
+ inflight batchingæš‚æœªå®ç°

æœ€åç»™å‡ºTable-1çš„è¡¥å……ï¼š
<div align=center>

<p>Table-2: Featureä¸Latencyçš„æ¶ˆèå®éªŒ(å¢åŠ int8 k/v cacheï¼Œsmoothquant, inflight batching)</p>

|Feature|åŸLlamaæ˜¯å¦å®ç°|æœ¬é¡¹ç›®æ˜¯å¦å¯ç”¨|batch size|input length|output length|åŠ é€Ÿæ¯”|
|-|-|-|-|-|-|-|
| K/V cache|âœ”ï¸|âœ”ï¸|1|8|50|-|
|+Attention Plugin|âœ”ï¸|âœ”ï¸|1|8|50|1.224|
|+Weight Only Quant|âœ”ï¸|âœ”ï¸|1|8|50|2.189|
|+Gemm Plugin|âœ”ï¸|âœ”ï¸|1|8|50|2.167|
|+Int4 Weight Only Quant|âœ”ï¸|âœ”ï¸|1|8|50|3.524|
|Int8 K/V cache|âŒ|âœ”ï¸|1|8|50|1.223|
|SmoothQuant|âŒ|âœ”ï¸|1|8|50|Build Error|
|Inflight Batching|âŒ|âŒ|1|8|50|-|

</div>

### 3.ä¼˜åŒ–æ•ˆæœ
---

#### 3.1 è¿è¡Œçš„è½¯ç¡¬ä»¶ç¯å¢ƒè¯´æ˜

æ•´ä¸ªé¡¹ç›®çš„æµ‹è¯•è½¯ç¡¬ä»¶ç¯å¢ƒå¦‚ä¸‹ï¼š

Hostç¡¬ä»¶ç¯å¢ƒï¼š
+ CPU: Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz
+ GPU: NVIDIA A10(24G) 
+ ç³»ç»Ÿï¼š Ubuntu 22.04.2 LTS

Hostè½¯ä»¶ç¯å¢ƒï¼š
+ æ˜¾å¡é©±åŠ¨ï¼šDriver Version: 525.105.17 
+ Dockerç‰ˆæœ¬ï¼š 24.0.5
+ NVIDIA-Docker

Dockeré•œåƒ:
+ å‚è€ƒé“¾æ¥ï¼š<https://github.com/NVIDIA/trt-samples-for-hackathon-cn/blob/master/Hackathon2023/HackathonGuide.md>
+ é•œåƒåä¸º: registry.cn-hangzhou.aliyuncs.com/trt-hackathon/trt-hackathon:final_v1
+ TensorRT 9.0 EA å®‰è£…ç›®å½•ä¸º: /usr/local/TensorRT-9.0.0.2
+ TensorRT-LLM ä»£ç ç›®å½•ä¸º /root/workspace/tensorrt_llm_july-release-v1

#### 3.2 æ¨ç†åŠ é€Ÿæ¯”å’Œæ¨¡å‹ç²¾åº¦å¯¹æ¯”

åŸºäºä¸Šè¿°è½¯ç¡¬ä»¶ç¯å¢ƒçš„æµ‹è¯•ï¼Œæˆ‘ä»¬ç»Ÿè®¡äº†section2ä¸­ç”Ÿæˆçš„ç»“æœï¼Œå› ä¸ºæ¨¡å‹æ˜¾å­˜é—®é¢˜ï¼Œæˆ‘ä»¬åªæä¾›`batch size=1`çš„ç»“æœï¼Œå…¶ä¸»è¦åŒ…æ‹¬åŠ é€Ÿæ¯”å’Œç²¾åº¦çš„æ¯”è¾ƒå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

<div align=center>

<!-- min input length 251 max input length 923 -->

<p>Table-3: LLaMA-7B TensorRT-LLMåŠ é€Ÿæ€§èƒ½ç»Ÿè®¡è¡¨ï¼ˆå»¶æ—¶æ¯”è¾ƒï¼‰</p>

|model|trick|max input length|output length| beam size|total latency(s)|speedup|
|-|-|-|-|-|-|-|
| HF Model|FP16|923|100|1|78.228|1.000|
| TRT Model|K/V cache++Attention Plugin(FP16)|923|100|1|66.031|1.185|
| TRT Model|+Weight Only Quant(int8)|923|100|1|40.297|1.941|
| TRT Model|+Gemm Plugin(int8)|923|100|1|41.237|1.897|
| TRT Model|+Weight Only Quant(int4)|923|100|1|28.596|2.736|
| TRT Model|Int8 K/V cache|923|100|1|66.319|1.180|

</div>


<div align=center>

<p>Table-4: LLaMA-7B TensorRT-LLMåŠ é€Ÿæ€§èƒ½ç»Ÿè®¡è¡¨ï¼ˆç²¾åº¦æ¯”è¾ƒï¼‰</p>

|model|trick|max input length|output length| beam size|rouge1 (abs-error)|rouge2(abs-error)|rougeL (abs-error)|rougeLsum (abs-error)|
|-|-|-|-|-|-|-|-|-|
| HF Model|FP16|923|100|1|20.106 (0.000)|5.910 (0.000)|15.226 (0.000)|17.938 (0.000)|
| TRT Model|K/V cache++Attention Plugin(FP16)|923|100|1|18.360 (1.746)|5.591 (0.319)|13.704 (1.522)|16.843 (1.095)|
| TRT Model|+Weight Only Quant(int8)|923|100|1|20.065 (0.041)|6.267 (0.357)|15.433 (0.207)|18.047 (0.109)|
| TRT Model|+Gemm Plugin(int8)|923|100|1|19.881 (0.225)|5.604 (0.306)|14.532 (0.694)|17.530 (0.408)|
| TRT Model|+Weight Only Quant(int4)|923|100|1|18.633 (1.473)|5.434 (0.476)|14.134 (1.092)|16.316 (1.622)|
| TRT Model|Int8 K/V cache|923|100|1|18.630 (1.476)|5.629 (0.281)|14.616 (0.610)|16.930 (1.008)|

</div>

è¯¥æµ‹è¯•æ•°æ®æ¥æºï¼š`cnn_dailymail`æ–‡æœ¬æ‘˜è¦æ•°æ®é›†ï¼Œåˆ†æä¸Šè¿°è¡¨æ ¼å‘ç°å¢åŠ ä¸åŒçš„trick,LLaMA-7Båœ¨TensorRT-LLMä¸‹çš„å»¶æ—¶å’Œç²¾åº¦çš„å˜åŒ–ï¼Œå¯ä»¥æ¸…æ™°çš„çœ‹åˆ°TensorRT-LLMå¯ä»¥å¾ˆå¥½çš„æŠŠrouge scoreçš„å·®å¼‚æ§åˆ¶åœ¨1ä»¥å†…æˆ–å·¦å³åŒæ—¶æœ‰`1.18-2.74`å€çš„åŠ é€Ÿæ¯”ï¼Œåœ¨int8 k/v cacheçš„æµ‹è¯•ç»“æœè¡¨æ˜æˆ‘ä»¬æ–°å¢çš„featureæ˜¯æ­£å¸¸å·¥ä½œçš„ä¸”æœ‰ä¸€å®šçš„åŠ é€Ÿæ•ˆæœã€‚æˆ‘ä»¬å°†æ¨¡å‹æµ‹è¯•è¿‡ç¨‹äº§ç”Ÿçš„æ—¥å¿—æºæ–‡ä»¶å­˜æ”¾åœ¨äº†`test_res`æ–‡ä»¶å¤¹ä¸‹ã€‚


### 4.BugæŠ¥å‘Š
---

<div align=center>

|:bug: Bugåç§°|Issue|æ˜¯å¦è¢«å®˜æ–¹ç¡®è®¤|è¯´æ˜|
|-|-|:-:|-|
|InstanceNormalization Plugin |<https://github.com/NVIDIA/TensorRT/issues/3165>||åˆèµ›æ—¶æäº¤åˆ°TensorRTï¼Œæˆ‘ä»¬ç¡®å®šæ˜¯bugä½†ä¸€ç›´æœªå¾—åˆ°å›å¤|

</div>

### 5.é€åˆ†é¢˜ç­”æ¡ˆ
---

> ğŸ” é—®é¢˜1ï¼šè¯·å†™å‡º `./tensorrt_llm_july-release-v1/examples/gpt/README` é‡Œé¢ `â€œSingle node, single GPUâ€` éƒ¨åˆ†å¦‚ä¸‹å‘½ä»¤çš„è¾“å‡ºï¼ˆ10åˆ†ï¼‰[æ¨¡å‹ä¸ºgpt2-medium](https://huggingface.co/gpt2-medium) 

```shell
python3 run.py --max_output_len=8 
```
<details>
<summary>ğŸ”‘ç‚¹æˆ‘æŸ¥çœ‹ é—®é¢˜1 è§£æ</summary>

0. å¿…è¦çš„Python Packageå®‰è£…

```shell
cd ./tensorrt_llm_july-release-v1/examples/gpt
pip3 install requirements.txt -i https://mirrors.aliyun.com/pypi/simple
```

1. ä¸‹è½½HuggingFace(HF)æ¨¡å‹

```shell
# ä¸‹è½½HFæ¨¡å‹
rm -rf gpt2 && git clone https://huggingface.co/gpt2-medium gpt2

# æ›´æ–°.binæ¨¡å‹
cd gpt2
rm pytorch_model.bin model.safetensors
wget https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin
cd ..
```
2. å°†HF weightè½¬ä¸ºFT weight

TensorRT-LLM å¯ä»¥ç›´æ¥åŠ è½½FastTransformer(FT)æ ¼å¼çš„æ¨¡å‹weightæ–‡ä»¶ï¼Œå› æ­¤éœ€è¦å°†HF weightè½¬æ¢ä¸ºFT weight

```shell
python3 hf_gpt_convert.py -i gpt2 -o ./c-model/gpt2 --tensor-parallelism 1 --storage-type float16
```
è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹è½¬æ¢å®Œæˆï¼Œå¹¶åœ¨`tensorrt_llm_july-release-v1/examples/gpt/c-model/gpt2/1-gpu`ä¸­å­˜æ”¾äº†ç”Ÿæˆåçš„FT weight

<div align=center>
<img src="./assets/section5/p1.png"/>
</div>

3. æ„å»ºTensorRT-LLM engine

TensorRT-LLM engineçš„æ„å»ºè¿‡ç¨‹ä½¿ç”¨äº†FT weightå’Œå¯¹åº”çš„é…ç½®æ–‡ä»¶ï¼ˆå·²ç»åœ¨ç¬¬2æ­¥ç”Ÿæˆï¼‰å’Œè‡ªå®šä¹‰çš„Tokenizerã€‚è¿‡ç¨‹ä¸­å¦‚æœä¸æŒ‡å®šæ¨¡å‹æƒé‡è·¯å¾„ï¼ŒTensorRT-LLMé»˜è®¤éšæœºåˆå§‹åŒ–è¿™äº›weightç”Ÿæˆengineã€‚

```shell
# single GPU float16 ä½¿ç”¨FT weightç”Ÿæˆengine
python3 build.py --model_dir=./c-model/gpt2/1-gpu --use_gpt_attention_plugin
```
æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹åºåˆ—åŒ–å®Œæˆï¼Œå¹¶å°†engineä¿å­˜åœ¨`./tensorrt_llm_july-release-v1/examples/gpt/gpt_outputs`ä¸­

```
[08/24/2023-07:07:38] [TRT-LLM] [I] Total time of building gpt_float16_tp1_rank0.engine: 00:00:35
[08/24/2023-07:07:38] [TRT-LLM] [I] Config saved to gpt_outputs/config.json.
[08/24/2023-07:07:38] [TRT-LLM] [I] Serializing engine to gpt_outputs/gpt_float16_tp1_rank0.engine...
[08/24/2023-07:07:42] [TRT-LLM] [I] Engine serialized. Total time: 00:00:04
[08/24/2023-07:07:42] [TRT-LLM] [I] Timing cache serialized to gpt_outputs/model.cache
[08/24/2023-07:07:42] [TRT-LLM] [I] Total time of building all 1 engines: 00:00:50
```
âš ï¸æ³¨æ„ï¼š `build.py`æ”¯æŒå¤šGPUå¹¶è¡Œæ„å»ºTensorRT-LLM engine,è¯¦ç»†çš„å¯ä»¥å‚è€ƒï¼š`./tensorrt_llm_july-release-v1/examples/gpt/README`

4. Single node,single GPUä¸‹æµ‹è¯•engine

```shell
# single GPUä¸‹è¿è¡Œengine
python3 run.py --max_output_len=8
```
è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```
Input: Born in north-east France, Soyer trained as a
Output:  chef and eventually became a chef at a
```
é—®é¢˜1è§£æå®Œæˆã€‚

âš ï¸æ³¨æ„ï¼š`run.py`æ”¯æŒsingle node multiple GPUs(åŸºäº`mpirun`)å’Œmultiple nodes,multiple GPUs(åŸºäº[Slurm](https://slurm.schedmd.com))ï¼Œè¯¦ç»†çš„å¯ä»¥å‚è€ƒ`./tensorrt_llm_july-release-v1/examples/gpt/README`

</details>


> ğŸ” é—®é¢˜2: è¯·å†™å‡º `./tensorrt_llm_july-release-v1/examples/gpt/README` é‡Œé¢ `â€œSummarization using the GPT modelâ€` éƒ¨åˆ†å¦‚ä¸‹å‘½ä»¤çš„rouge åˆ†æ•°ï¼ˆ10åˆ†ï¼‰[æ¨¡å‹ä¸ºgpt2-medium](https://huggingface.co/gpt2-medium)

```shell
python3 summarize.py --engine_dirtrt_engine/gpt2/fp16/1-gpu --test_hf  --batch_size1  --test_trt_llm  --hf_model_location=gpt2 --check_accuracy --tensorrt_llm_rouge1_threshold=14
```

<details>
<summary>ğŸ”‘ç‚¹æˆ‘æŸ¥çœ‹ é—®é¢˜2 è§£æ</summary>

è¯¥é—®é¢˜å°†æè¿°å¦‚ä½•ä½¿ç”¨TensorRT-LLMè¿è¡Œä¸€ä¸ªæ–‡æœ¬æ‘˜è¦ä»»åŠ¡çš„GPT-2,è¿™é‡Œä½¿ç”¨çš„æ•°æ®é›†ä¸º[cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) ï¼Œå¯¹åº”ç”Ÿæˆçš„æ‘˜è¦ä½¿ç”¨[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) scoreæ¥è¯„ä»·TensorRT-LLMçš„ç²¾åº¦å˜åŒ–ï¼Œç¡®åˆ‡çš„è¯´è¿™é‡Œä½¿ç”¨äº†`ROUGE-1` scoreã€‚

å…³äºè¯„ä»·æŒ‡æ ‡[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric))çš„ä»‹ç»ï¼Œæˆ‘ä»¬æ¨èå‚è€ƒçŸ¥ä¹çš„ä»‹ç»ï¼š

+ [ä¸­æ–‡æ–‡æœ¬æ‘˜è¦æŒ‡æ ‡-ROUGE](https://zhuanlan.zhihu.com/p/388720967)
+ [NLPè¯„ä¼°æŒ‡æ ‡ä¹‹ROUGE](https://zhuanlan.zhihu.com/p/504279252)

é—®é¢˜1ä¸­å·²ç»å®Œæˆäº†å¿…è¦çš„packageçš„å®‰è£…ï¼Œè¿™é‡Œç›´æ¥ä¸‹è½½HFæ¨¡å‹

1. ä¸‹è½½HFæ¨¡å‹æ–‡ä»¶

```shell
# ä¸‹è½½æ¨¡å‹æ–‡ä»¶
rm -rf gpt2 && git clone https://huggingface.co/gpt2 gpt2

# æ›´æ–°.binæ¨¡å‹æ–‡ä»¶
cd gpt2
rm pytorch_model.bin model.safetensors
wget https://huggingface.co/gpt2/resolve/main/pytorch_model.bin
cd ..
```

2. å°†HF weightè½¬æ¢ä¸ºFT weight

```shell
python3 hf_gpt_convert.py -i gpt2 -o ./c-model/gpt2/fp16 --tensor-parallelism 1 --storage-type float16
```

è¿è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹å›¾æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹è½¬æ¢å®Œæˆï¼Œå¹¶åœ¨`tensorrt_llm_july-release-v1/examples/gpt/c-model/gpt2/fp16/1-gpu`ä¸­å­˜æ”¾äº†ç”Ÿæˆåçš„FT weight

<div align=center>
<img src="./assets/section5/p2.png"/>
</div>

3. æ„å»ºTensorRT-LLM engine

```shell
python3 build.py --model_dir=./c-model/gpt2/fp16/1-gpu \
                 --use_gpt_attention_plugin \
                 --use_gemm_plugin \
                 --use_layernorm_plugin \
                 --max_batch_size 8 \
                 --max_input_len 924 \
                 --max_output_len 100 \
                 --output_dir trt_engine/gpt2/fp16/1-gpu/ \
                 --hidden_act gelu
```

æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œlogä¸­å‡ºç°å¦‚ä¸‹æ‰€ç¤ºç»“æœï¼Œè¯´æ˜æ¨¡å‹åºåˆ—åŒ–å®Œæˆï¼Œå¹¶å°†engineä¿å­˜åœ¨`./tensorrt_llm_july-release-v1/examples/gpt/trt_engine/gpt2/fp16/1-gpu`ä¸­

```
[08/24/2023-07:34:42] [TRT-LLM] [I] Total time of building gpt_float16_tp1_rank0.engine: 00:01:32
[08/24/2023-07:34:42] [TRT-LLM] [I] Config saved to trt_engine/gpt2/fp16/1-gpu/config.json.
[08/24/2023-07:34:42] [TRT-LLM] [I] Serializing engine to trt_engine/gpt2/fp16/1-gpu/gpt_float16_tp1_rank0.engine...
[08/24/2023-07:34:43] [TRT-LLM] [I] Engine serialized. Total time: 00:00:01
[08/24/2023-07:34:43] [TRT-LLM] [I] Timing cache serialized to trt_engine/gpt2/fp16/1-gpu/model.cache
[08/24/2023-07:34:43] [TRT-LLM] [I] Total time of building all 1 engines: 00:01:43
```

4. TensorRT-LLMä¸‹æµ‹è¯•GPT-2æ–‡æœ¬æ‘˜è¦ä»»åŠ¡

```shell
python3 summarize.py --engine_dir trt_engine/gpt2/fp16/1-gpu \
                     --test_hf \
                     --batch_size 1 \
                     --test_trt_llm \
                     --hf_model_location=gpt2 \
                     --check_accuracy \
                     --tensorrt_llm_rouge1_threshold=14
```
æ‰§è¡Œä¸Šè¿°ä»£ç ï¼Œç»“æœå¦‚ä¸‹ï¼š

```
[08/24/2023-08:40:54] [TRT-LLM] [I] ---------------------------------------------------------
Downloading builder script: 5.60kB [00:00, 6.22MB/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1151 > 1024). Running this sequence through the model will result in indexing errors
[08/24/2023-08:41:14] [TRT-LLM] [I] TensorRT-LLM (total latency: 2.6481666564941406 sec)
[08/24/2023-08:41:14] [TRT-LLM] [I] TensorRT-LLM beam 0 result
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge1 : 15.361040799540035
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge2 : 3.854022269668396
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeL : 12.078455591738333
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeLsum : 13.547802733617264
[08/24/2023-08:41:14] [TRT-LLM] [I] Hugging Face (total latency: 10.39808702468872 sec)
[08/24/2023-08:41:14] [TRT-LLM] [I] HF beam 0 result
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge1 : 14.75593024343394
[08/24/2023-08:41:14] [TRT-LLM] [I]   rouge2 : 3.3647470801871733
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeL : 11.124766996533
[08/24/2023-08:41:14] [TRT-LLM] [I]   rougeLsum : 13.031128048110618
```

<div align=center>
<img src="./assets/section5/p3.png"/>
</div>

é—®é¢˜2è§£æå®Œæˆã€‚

âš ï¸æ³¨æ„ï¼šè¿‡ç¨‹ä¸­éœ€è¦ä¸‹è½½æ•°æ®ï¼Œå¦‚æœè¿è¡Œè¿‡ç¨‹ä¸­ä¸­æ–­æˆ–æŠ¥é”™å¤§å¤šæ˜¯å› ä¸ºç½‘ç»œåŸå› ï¼Œè¯·è€å¿ƒå¤šå°è¯•é‡å¤è¿è¡Œå‡ æ¬¡

</details>


### 6.æœªæ¥å·¥ä½œ
---

LLMå¯ä»¥è‚¯å®šçš„æ˜¯å½“å‰å’Œæœªæ¥AIæ·±å…¥ç ”ç©¶å’Œäº§å“åŒ–çš„é‡è¦ç ”ç©¶æ–¹å‘ï¼Œè¶Šæ¥è¶Šå¤šçš„ä¼ä¸šå’Œç ”ç©¶æœºæ„ä¹Ÿåœ¨è‡´åŠ›ç ”ç©¶LLMçš„åŠ é€Ÿæ¨ç†æ¡†æ¶ï¼ŒTensorRT-LLMä½œä¸ºæ–°ä¸€ä»£çš„åŸºäºTensorRT 9.0çš„LLMçš„æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œç»™å¤§æ¨¡å‹çš„éƒ¨ç½²æä¾›äº†é«˜æ•ˆå¯è¡Œçš„æ–¹æ¡ˆã€‚TensorRT-LLMé›†æˆäº†ä¸»æµçš„LLMæ¨¡å‹åŠ é€Ÿçš„æ–¹æ¡ˆåŒ…æ‹¬ï¼šï¼ˆint8) k/v cache, weight only quant, smooth quant, inflight batchingåŠå„ç§é«˜æ•ˆçš„Pluginã€‚æ¯”èµ›è¿‡ç¨‹ä¸­æˆ‘ä»¬éœ€è¦ç»†è‡´çš„ç ”ç©¶TensorRT-LLMçš„å®ç°æ–¹å¼ï¼Œç†Ÿæ‚‰TenserRT-LLMçš„æ„å»ºé€»è¾‘ï¼Œè¿™èŠ±è´¹æˆ‘ä»¬å¤§é‡çš„æ—¶é—´å’Œç²¾åŠ›ï¼ŒåŠ ä¹‹å¯¹TensorRT-LLMçš„ç†Ÿæ‚‰ç¨‹åº¦ä¸å¤Ÿå¯¼è‡´æˆ‘ä»¬æ— æ³•åœ¨çŸ­æ—¶é—´å®ç°æˆ‘ä»¬è§„åˆ’çš„æ–°Featureçš„å®ç°ï¼Œæ¯”å¦‚smoothquantçš„build engineçš„erroré—®é¢˜ï¼Œinflight batchingçš„featureçš„å®ç°ç­‰ç­‰ã€‚æœªæ¥æˆ‘ä»¬å°†æŒç»­å…³æ³¨TensorRT-LLMçš„è¿›å±•ï¼Œå®ç°ä¸Šè¿°é—ç•™çš„é—®é¢˜ï¼Œå¹¶æœŸå¾…æ­£å¼ç‰ˆæœ¬çš„TensorRT-LLMçš„å‘å¸ƒã€‚

### 7.ğŸ˜‰References
---

1. [TensorRT(Github)](https://github.com/NVIDIA/TensorRT)

2. [trt-samples-for-hackathon-cn](https://github.com/NVIDIA/trt-samples-for-hackathon-cn)

3. [NVIDIA TensorRT Hackathon 2023 â€”â€” ç”Ÿæˆå¼AIæ¨¡å‹ä¼˜åŒ–èµ›(å¤©æ± )](https://tianchi.aliyun.com/competition/entrance/532108/introduction?spm=a2c22.12281957.0.0.605a3b74bkLhBT)

4. [TensorRT 8.6 è®²åº§(Bç«™)](https://www.bilibili.com/video/BV1jj411Z7wG/)

5. [Llama(Github)](https://github.com/facebookresearch/llama)

6. [Llama paper (arxiv)](https://arxiv.org/abs/2302.13971)

7. [TensorRT-LLM:å¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼šä½ç²¾åº¦æœ€ä½³å®è·µ(Bç«™)](https://www.bilibili.com/video/BV1h44y1c72B/?share_source=copy_web&vd_source=db3eecb1b88cc6c7a18eeaf6db1ed114)

8. [TensorRT-LLMå¤§è¯­è¨€æ¨¡å‹æ¨ç†ï¼šä¼˜åŒ–å…³é”®æŠ€æœ¯è§£æ(Bç«™)](https://www.bilibili.com/video/BV1j44y1c7fT/?spm_id_from=333.788&vd_source=def8c63d9c5f9bf987870bf827bfcb3d)

9. [SmoothQuant(arxiv)](https://arxiv.org/abs/2211.10438)


